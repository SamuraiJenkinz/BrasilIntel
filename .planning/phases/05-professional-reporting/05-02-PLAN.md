---
phase: 05-professional-reporting
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/executive_summarizer.py
  - app/schemas/report.py
autonomous: true

must_haves:
  truths:
    - "Azure OpenAI generates executive summary paragraph for report"
    - "Summary is 2-3 sentences in Portuguese"
    - "Service returns fallback text when LLM unavailable"
    - "Context preparation limits token usage"
  artifacts:
    - path: "app/services/executive_summarizer.py"
      provides: "AI-powered executive summary generation"
      min_lines: 80
      contains: "ExecutiveSummarizer"
    - path: "app/schemas/report.py"
      provides: "Pydantic schema for structured output"
      contains: "ExecutiveSummary"
  key_links:
    - from: "app/services/executive_summarizer.py"
      to: "app/config.py"
      via: "get_settings()"
      pattern: "azure_openai_endpoint"
    - from: "app/services/executive_summarizer.py"
      to: "openai.AzureOpenAI"
      via: "client.beta.chat.completions.parse"
      pattern: "response_format=ExecutiveSummary"
---

<objective>
Create AI-powered executive summary generation service using Azure OpenAI structured outputs.

Purpose: Generate concise Portuguese executive summary paragraphs for each report based on insurer data and news items.
Output: ExecutiveSummarizer service class with Pydantic schema for structured outputs.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-professional-reporting/05-RESEARCH.md
@app/services/classifier.py (reference for Azure OpenAI patterns)
@app/config.py (Azure OpenAI configuration)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Pydantic schema for executive summary structured output</name>
  <files>app/schemas/report.py</files>
  <action>
Create `app/schemas/report.py` with Pydantic models for report-related schemas.

```python
"""
Pydantic schemas for report generation.

Includes structured output schemas for Azure OpenAI executive summary generation.
"""
from pydantic import BaseModel, Field
from typing import Literal


class ExecutiveSummary(BaseModel):
    """
    Structured output schema for Azure OpenAI executive summary generation.

    Used with client.beta.chat.completions.parse() for guaranteed schema conformance.
    """
    paragraph: str = Field(
        description="2-3 sentence executive summary in Portuguese highlighting key market developments"
    )
    critical_count: int = Field(
        description="Number of insurers with Critical status in the report"
    )
    watch_count: int = Field(
        description="Number of insurers with Watch status in the report"
    )
    key_theme: Literal["turbulencia", "estabilidade", "crescimento", "consolidacao", "crise", "regulatorio"] = Field(
        description="One-word Portuguese theme summarizing the market situation"
    )


class KeyFinding(BaseModel):
    """Schema for key findings cards in executive summary."""
    severity: Literal["critical", "warning", "positive"] = Field(
        description="Visual severity level for card styling"
    )
    title: str = Field(
        description="Brief title for the finding (max 50 chars)"
    )
    description: str = Field(
        description="1-2 sentence description of the finding"
    )


class ReportContext(BaseModel):
    """Schema for market context items."""
    title: str = Field(
        description="Context item title"
    )
    description: str = Field(
        description="Context item description"
    )
```
  </action>
  <verify>
File exists: `test -f app/schemas/report.py && echo "Schema file exists"`
Contains required classes: `grep -c "class ExecutiveSummary\|class KeyFinding\|class ReportContext" app/schemas/report.py` returns 3
Has Pydantic Field: `grep -c "Field(" app/schemas/report.py` returns >= 6
  </verify>
  <done>
Schema file exists with ExecutiveSummary, KeyFinding, and ReportContext Pydantic models with proper field descriptions for Azure OpenAI structured outputs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ExecutiveSummarizer service</name>
  <files>app/services/executive_summarizer.py</files>
  <action>
Create `app/services/executive_summarizer.py` implementing AI-powered summary generation.

```python
"""
AI-powered executive summary generation using Azure OpenAI.

Generates concise Portuguese executive summaries for insurance intelligence reports
using structured outputs with Pydantic schemas.
"""
import time
from typing import Optional
import structlog

from openai import AzureOpenAI

from app.config import get_settings
from app.models.insurer import Insurer
from app.schemas.report import ExecutiveSummary, KeyFinding

logger = structlog.get_logger()


class ExecutiveSummarizer:
    """
    Service for generating AI-powered executive summaries.

    Uses Azure OpenAI gpt-4o with structured outputs to generate
    consistent, schema-conformant summaries in Portuguese.
    """

    def __init__(self):
        """Initialize Azure OpenAI client with settings."""
        self.settings = get_settings()
        self.client = AzureOpenAI(
            api_key=self.settings.azure_openai_key,
            api_version="2024-08-01-preview",
            azure_endpoint=self.settings.azure_openai_endpoint
        )
        self.model = self.settings.azure_openai_deployment

    def generate_executive_summary(
        self,
        category: str,
        insurers: list[Insurer],
        max_retries: int = 3
    ) -> str:
        """
        Generate executive summary paragraph for report.

        Args:
            category: Report category (Health, Dental, Group Life)
            insurers: List of insurers with loaded news_items
            max_retries: Maximum retry attempts on failure

        Returns:
            Portuguese executive summary paragraph (2-3 sentences)
        """
        # Prepare context from insurers
        context_text = self._prepare_context(category, insurers)

        # System prompt in Portuguese for Brazilian executives
        system_prompt = """Voce e um analista senior da Marsh Brasil especializado em inteligencia de mercado de seguros.

Gere um paragrafo executivo conciso (2-3 frases) em portugues que resuma os principais desenvolvimentos afetando as seguradoras listadas.

Foco em:
- Tendencias criticas do mercado
- Mudancas regulatorias importantes
- Riscos financeiros ou operacionais
- Oportunidades estrategicas

Escreva em portugues profissional para executivos. Seja direto e objetivo."""

        retry_count = 0
        while retry_count < max_retries:
            try:
                response = self.client.beta.chat.completions.parse(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": context_text}
                    ],
                    response_format=ExecutiveSummary,
                    temperature=0.3,  # Lower for consistency
                    max_tokens=500,
                    timeout=30.0
                )

                summary = response.choices[0].message.parsed

                logger.info(
                    "executive_summary_generated",
                    category=category,
                    critical_count=summary.critical_count,
                    watch_count=summary.watch_count,
                    theme=summary.key_theme
                )

                return summary.paragraph

            except Exception as e:
                retry_count += 1
                logger.warning(
                    "executive_summary_retry",
                    attempt=retry_count,
                    error=str(e),
                    category=category
                )

                if retry_count >= max_retries:
                    logger.error(
                        "executive_summary_failed",
                        category=category,
                        error=str(e)
                    )
                    return self._generate_fallback_summary(category, insurers)

                # Exponential backoff
                time.sleep(2 ** retry_count)

        return self._generate_fallback_summary(category, insurers)

    def generate_key_findings(
        self,
        insurers_by_status: dict[str, list[Insurer]]
    ) -> list[KeyFinding]:
        """
        Generate key findings cards from insurer data.

        Creates 3-4 findings based on critical/watch insurers and their news.
        Does not use LLM - derives from data directly.

        Args:
            insurers_by_status: Insurers grouped by status

        Returns:
            List of KeyFinding objects for template rendering
        """
        findings = []

        # Critical findings
        critical_insurers = insurers_by_status.get("Critical", [])
        if critical_insurers:
            # Get most severe critical news
            critical_names = [i.name for i in critical_insurers[:2]]
            findings.append(KeyFinding(
                severity="critical",
                title=f"Alerta Critico - {len(critical_insurers)} Seguradora(s)",
                description=f"Seguradoras em situacao critica: {', '.join(critical_names)}. Requer atencao imediata."
            ))

        # Watch findings
        watch_insurers = insurers_by_status.get("Watch", [])
        if watch_insurers:
            watch_names = [i.name for i in watch_insurers[:2]]
            findings.append(KeyFinding(
                severity="warning",
                title=f"Monitoramento Ativo - {len(watch_insurers)} Seguradora(s)",
                description=f"Seguradoras sob observacao: {', '.join(watch_names)}. Acompanhar desenvolvimentos."
            ))

        # Positive findings (from stable with good news)
        stable_insurers = insurers_by_status.get("Stable", [])
        if stable_insurers:
            # Find insurers with positive sentiment news
            positive_insurers = []
            for insurer in stable_insurers:
                if any(n.sentiment == "positive" for n in insurer.news_items if n.sentiment):
                    positive_insurers.append(insurer)

            if positive_insurers:
                positive_names = [i.name for i in positive_insurers[:2]]
                findings.append(KeyFinding(
                    severity="positive",
                    title=f"Desenvolvimentos Positivos",
                    description=f"Noticias favoraveis para: {', '.join(positive_names)}."
                ))

        # Monitor summary
        monitor_insurers = insurers_by_status.get("Monitor", [])
        if monitor_insurers and len(findings) < 4:
            findings.append(KeyFinding(
                severity="warning",
                title=f"Em Acompanhamento - {len(monitor_insurers)} Seguradora(s)",
                description=f"{len(monitor_insurers)} seguradoras requerem monitoramento continuo."
            ))

        return findings

    def _prepare_context(self, category: str, insurers: list[Insurer]) -> str:
        """
        Prepare concise context for LLM from insurers.

        Limits token usage by including only key information:
        - Insurer name and status
        - Top 2 critical/watch news items per insurer

        Args:
            category: Report category
            insurers: List of insurers

        Returns:
            Formatted context string
        """
        status_priority = ["Critical", "Watch", "Monitor", "Stable"]

        lines = [f"Categoria: {category}\nSeguradoras analisadas:\n"]

        for insurer in insurers:
            # Determine status from news items
            statuses = [n.status for n in insurer.news_items if n.status]
            if not statuses:
                continue

            # Use most severe status
            status = "Stable"
            for s in status_priority:
                if s in statuses:
                    status = s
                    break

            lines.append(f"\n{insurer.name} (ANS {insurer.ans_code}): {status}")

            # Include top 2 critical/watch news items for context
            important_news = [
                n for n in insurer.news_items
                if n.status in ["Critical", "Watch"]
            ][:2]

            for news in important_news:
                lines.append(f"  - {news.title}")
                if news.category_indicators:
                    indicators = news.category_indicators.split(",") if isinstance(news.category_indicators, str) else news.category_indicators
                    lines.append(f"    Indicadores: {', '.join(indicators[:3])}")

        return "\n".join(lines)

    def _generate_fallback_summary(
        self,
        category: str,
        insurers: list[Insurer]
    ) -> str:
        """
        Generate template-based fallback summary when LLM unavailable.

        Args:
            category: Report category
            insurers: List of insurers

        Returns:
            Basic Portuguese summary paragraph
        """
        # Count statuses
        status_counts = {"Critical": 0, "Watch": 0, "Monitor": 0, "Stable": 0}
        for insurer in insurers:
            statuses = [n.status for n in insurer.news_items if n.status]
            if "Critical" in statuses:
                status_counts["Critical"] += 1
            elif "Watch" in statuses:
                status_counts["Watch"] += 1
            elif "Monitor" in statuses:
                status_counts["Monitor"] += 1
            else:
                status_counts["Stable"] += 1

        total = len(insurers)
        critical = status_counts["Critical"]
        watch = status_counts["Watch"]

        if critical > 0:
            return (
                f"Este relatorio analisa {total} seguradoras na categoria {category}. "
                f"Identificamos {critical} seguradora(s) em situacao critica e {watch} sob monitoramento ativo. "
                f"Recomenda-se atencao prioritaria aos casos criticos destacados neste documento."
            )
        elif watch > 0:
            return (
                f"Este relatorio analisa {total} seguradoras na categoria {category}. "
                f"Identificamos {watch} seguradora(s) sob monitoramento ativo. "
                f"O mercado apresenta estabilidade geral com pontos de atencao especificos."
            )
        else:
            return (
                f"Este relatorio analisa {total} seguradoras na categoria {category}. "
                f"O mercado apresenta estabilidade geral sem alertas criticos no periodo analisado."
            )
```
  </action>
  <verify>
File exists: `test -f app/services/executive_summarizer.py && echo "Service file exists"`
Contains class: `grep -c "class ExecutiveSummarizer" app/services/executive_summarizer.py` returns 1
Uses Azure OpenAI: `grep -c "AzureOpenAI\|beta.chat.completions.parse" app/services/executive_summarizer.py` returns >= 2
Has fallback: `grep -c "_generate_fallback_summary" app/services/executive_summarizer.py` returns >= 2
Has retry logic: `grep -c "retry_count\|max_retries" app/services/executive_summarizer.py` returns >= 3
  </verify>
  <done>
ExecutiveSummarizer service exists with:
- Azure OpenAI integration using structured outputs
- Portuguese system prompts for Brazilian executives
- Retry logic with exponential backoff
- Fallback summary when LLM unavailable
- Token-efficient context preparation
- Key findings generation from data
  </done>
</task>

</tasks>

<verification>
1. Schema file imports successfully: `python -c "from app.schemas.report import ExecutiveSummary, KeyFinding"`
2. Service file imports successfully: `python -c "from app.services.executive_summarizer import ExecutiveSummarizer"`
3. Service initializes with settings: `python -c "from app.services.executive_summarizer import ExecutiveSummarizer; s = ExecutiveSummarizer(); print('OK')"`
</verification>

<success_criteria>
- ExecutiveSummarizer class instantiates without errors
- Pydantic schemas validate correctly
- Fallback summary generates valid Portuguese text
- Code follows existing classifier.py patterns
</success_criteria>

<output>
After completion, create `.planning/phases/05-professional-reporting/05-02-SUMMARY.md`
</output>
