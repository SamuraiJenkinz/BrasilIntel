---
phase: 03-news-collection-scale
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/sources/__init__.py
  - app/services/sources/base.py
  - app/services/sources/google_news.py
autonomous: true

must_haves:
  truths:
    - "All news sources implement a common interface"
    - "Sources can be discovered via registry"
    - "Google News source works exactly as before"
  artifacts:
    - path: "app/services/sources/base.py"
      provides: "NewsSource ABC and SourceRegistry"
      exports: ["NewsSource", "SourceRegistry", "ScrapedNewsItem"]
    - path: "app/services/sources/google_news.py"
      provides: "Google News source implementation"
      exports: ["GoogleNewsSource"]
    - path: "app/services/sources/__init__.py"
      provides: "Package exports"
      exports: ["NewsSource", "SourceRegistry", "GoogleNewsSource"]
  key_links:
    - from: "app/services/sources/google_news.py"
      to: "app/services/sources/base.py"
      via: "class inheritance"
      pattern: "class GoogleNewsSource\\(NewsSource\\)"
    - from: "app/services/sources/google_news.py"
      to: "apify_client"
      via: "ApifyClient import"
      pattern: "from apify_client import ApifyClient"
---

<objective>
Create the news source abstraction layer with abstract base class, source registry, and refactor existing Google News scraper into the new pattern.

Purpose: Enable multiple news sources with different implementations (RSS, Apify actors, web crawlers) to be used interchangeably through a common interface. This is the foundation for scaling to 6 news sources.

Output: Source abstraction module with registry pattern and Google News implementation matching existing behavior.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-news-collection-scale/03-RESEARCH.md

# Existing implementation to refactor
@app/services/scraper.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create source abstraction base module</name>
  <files>
    app/services/sources/__init__.py
    app/services/sources/base.py
  </files>
  <action>
Create new `app/services/sources/` package with:

**base.py:**
1. Move `ScrapedNewsItem` class from scraper.py into base.py (keep as dataclass with same fields: title, description, url, source, published_at, raw_data)
2. Create `NewsSource` abstract base class with:
   - `SOURCE_NAME: ClassVar[str]` - class variable for source identification
   - `async def search(self, query: str, max_results: int = 10) -> list[ScrapedNewsItem]` - abstract method
   - `async def health_check(self) -> dict` - abstract method returning {"status": "ok"} or {"status": "error", "message": "..."}
3. Create `SourceRegistry` class with:
   - `_sources: dict[str, NewsSource]` class variable
   - `@classmethod def register(cls, source: NewsSource)` - adds source to registry
   - `@classmethod def get(cls, name: str) -> NewsSource | None` - get source by name
   - `@classmethod def get_all(cls) -> list[NewsSource]` - return all registered sources
   - `@classmethod def names(cls) -> list[str]` - return all source names

**__init__.py:**
Export: NewsSource, SourceRegistry, ScrapedNewsItem

Use `from __future__ import annotations` for forward references. Import ABC, abstractmethod from abc. Import ClassVar from typing. Import datetime from datetime. Import dataclass from dataclasses.
  </action>
  <verify>
```bash
python -c "from app.services.sources import NewsSource, SourceRegistry, ScrapedNewsItem; print('Imports OK')"
python -c "from app.services.sources.base import NewsSource; print(NewsSource.__abstractmethods__)"
```
Verify: Imports succeed, abstract methods listed as {'search', 'health_check'}
  </verify>
  <done>
- NewsSource ABC exists with search() and health_check() abstract methods
- SourceRegistry provides register/get/get_all/names class methods
- ScrapedNewsItem dataclass moved to base.py
- Package exports work correctly
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor Google News into source pattern</name>
  <files>app/services/sources/google_news.py</files>
  <action>
Create `google_news.py` implementing NewsSource:

1. Import NewsSource, SourceRegistry, ScrapedNewsItem from .base
2. Import ApifyClient from apify_client
3. Import get_settings from app.config
4. Import logging, datetime

Create `GoogleNewsSource(NewsSource)`:
- `SOURCE_NAME = "google_news"`
- `ACTOR_ID = "lhotanova/google-news-scraper"`
- `__init__(self)`: Initialize ApifyClient from settings.apify_token (set self.client = None if not configured)
- `async def search(self, query: str, max_results: int = 10) -> list[ScrapedNewsItem]`:
  - Return [] if self.client is None
  - Build run_input with queries=query, language="pt", country="BR", maxItems=max_results, timeRange="7d"
  - Call self.client.actor(self.ACTOR_ID).call(run_input=run_input, timeout_secs=300)
  - Iterate dataset items and parse into ScrapedNewsItem list using existing _parse_results logic
  - Wrap in try/except, return [] on failure
- `async def health_check(self) -> dict`:
  - Return {"status": "error", "message": "APIFY_TOKEN not configured"} if no client
  - Try self.client.user().get() and return {"status": "ok", "username": user["username"]}
  - On exception return {"status": "error", "message": str(e)}
- `def _parse_results(self, items: list[dict]) -> list[ScrapedNewsItem]`: Copy existing logic from scraper.py
- `def search_insurer(self, insurer_name: str, ans_code: str, max_results: int = 10) -> list[ScrapedNewsItem]`: Convenience method building query as `"{insurer_name}" OR "ANS {ans_code}"` and calling search()

At module level, auto-register: `SourceRegistry.register(GoogleNewsSource())`

Note: The search method is marked async for interface consistency but internally uses sync Apify client.call(). This is intentional - the batch processor will handle async orchestration.
  </action>
  <verify>
```bash
python -c "
from app.services.sources import SourceRegistry, GoogleNewsSource
sources = SourceRegistry.get_all()
print(f'Registered sources: {len(sources)}')
print(f'Google News: {SourceRegistry.get(\"google_news\") is not None}')
gs = GoogleNewsSource()
print(f'SOURCE_NAME: {gs.SOURCE_NAME}')
"
```
Verify: 1 source registered, Google News found, SOURCE_NAME is "google_news"
  </verify>
  <done>
- GoogleNewsSource implements NewsSource interface
- Auto-registers with SourceRegistry on import
- search() method returns list[ScrapedNewsItem]
- health_check() returns status dict
- search_insurer() convenience method works
- Behavior matches existing ApifyScraperService
  </done>
</task>

<task type="auto">
  <name>Task 3: Update scraper.py to use new source</name>
  <files>app/services/scraper.py</files>
  <action>
Update scraper.py to import from sources module while maintaining backward compatibility:

1. Remove ScrapedNewsItem class definition (now in sources/base.py)
2. Add import: `from app.services.sources import ScrapedNewsItem, GoogleNewsSource, SourceRegistry`
3. Update ApifyScraperService to delegate to GoogleNewsSource internally:
   - In __init__, create self._google_source = GoogleNewsSource()
   - Update search_google_news() to call self._google_source methods
   - Update health_check() to delegate to self._google_source.health_check()
4. Keep ScrapedNewsItem re-exported for backward compatibility: `from app.services.sources import ScrapedNewsItem` at top level

The goal is zero breaking changes to existing code that imports from scraper.py. The ApifyScraperService class remains functional and is the entry point for Phase 2 code.
  </action>
  <verify>
```bash
python -c "
from app.services.scraper import ApifyScraperService, ScrapedNewsItem
service = ApifyScraperService()
print(f'Service created: {service is not None}')
print(f'ScrapedNewsItem importable: {ScrapedNewsItem is not None}')
hc = service.health_check()
print(f'Health check: {hc}')
"
```
Verify: Service creates, ScrapedNewsItem imports, health_check returns dict
  </verify>
  <done>
- ScrapedNewsItem importable from both scraper.py and sources module
- ApifyScraperService works exactly as before
- Internal delegation to GoogleNewsSource
- Zero breaking changes to runs.py or other consumers
  </done>
</task>

</tasks>

<verification>
Run existing tests and verify no regressions:
```bash
# Module imports
python -c "from app.services.scraper import ApifyScraperService, ScrapedNewsItem; print('scraper imports OK')"
python -c "from app.services.sources import NewsSource, SourceRegistry, ScrapedNewsItem, GoogleNewsSource; print('sources imports OK')"

# Registry functionality
python -c "
from app.services.sources import SourceRegistry
print(f'Sources: {SourceRegistry.names()}')
print(f'Count: {len(SourceRegistry.get_all())}')
"

# Backward compatibility
python -c "
from app.routers.runs import router
print('runs.py imports without error')
"
```
</verification>

<success_criteria>
1. NewsSource ABC defines search() and health_check() abstract methods
2. SourceRegistry provides source registration and discovery
3. GoogleNewsSource implements NewsSource and auto-registers
4. ApifyScraperService continues to work unchanged
5. ScrapedNewsItem importable from both old and new locations
6. All existing code (runs.py, etc.) continues to function
</success_criteria>

<output>
After completion, create `.planning/phases/03-news-collection-scale/03-01-SUMMARY.md`
</output>
