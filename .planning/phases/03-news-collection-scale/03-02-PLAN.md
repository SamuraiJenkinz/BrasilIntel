---
phase: 03-news-collection-scale
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - app/services/sources/rss_source.py
  - app/services/sources/infomoney.py
  - app/services/sources/estadao.py
  - app/services/sources/ans.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "System scrapes InfoMoney RSS feeds for insurance news"
    - "System scrapes Estadao RSS feeds for insurance news"
    - "System scrapes ANS official releases from gov.br"
    - "RSS sources filter by query keywords"
  artifacts:
    - path: "app/services/sources/rss_source.py"
      provides: "Generic RSS feed scraper"
      exports: ["RSSNewsSource"]
    - path: "app/services/sources/infomoney.py"
      provides: "InfoMoney source"
      exports: ["InfoMoneySource"]
    - path: "app/services/sources/estadao.py"
      provides: "Estadao source"
      exports: ["EstadaoSource"]
    - path: "app/services/sources/ans.py"
      provides: "ANS official releases source"
      exports: ["ANSSource"]
  key_links:
    - from: "app/services/sources/infomoney.py"
      to: "app/services/sources/rss_source.py"
      via: "class inheritance"
      pattern: "class InfoMoneySource\\(RSSNewsSource\\)"
    - from: "app/services/sources/rss_source.py"
      to: "feedparser"
      via: "RSS parsing"
      pattern: "feedparser\\.parse"
---

<objective>
Implement RSS-based news sources for InfoMoney, Estadao, and ANS using feedparser.

Purpose: Add 3 of the 5 additional news sources required by Phase 3. RSS feeds provide reliable, structured data with consistent formatting. These sources handle NEWS-03 (InfoMoney), NEWS-06 (Broadcast/Estadao), and NEWS-05 (ANS).

Output: Generic RSS source base class and 3 source implementations, all registered with SourceRegistry.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-news-collection-scale/03-RESEARCH.md
@.planning/phases/03-news-collection-scale/03-01-SUMMARY.md

# Source abstraction from Plan 01
@app/services/sources/base.py
@app/services/sources/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add feedparser and aiohttp dependencies</name>
  <files>requirements.txt</files>
  <action>
Add to requirements.txt under a new "Phase 3 - News Collection Scale" section:

```
# Phase 3 - News Collection Scale
feedparser>=6.0.0
aiohttp>=3.9.0
```

feedparser: Industry-standard RSS/Atom feed parser that handles malformed XML, date formats, encoding issues.
aiohttp: Async HTTP client for fetching feeds without blocking.

Place after the Phase 2 section, before any dev dependencies if present.
  </action>
  <verify>
```bash
pip install feedparser aiohttp
python -c "import feedparser; import aiohttp; print('Dependencies installed')"
```
  </verify>
  <done>
- feedparser>=6.0.0 in requirements.txt
- aiohttp>=3.9.0 in requirements.txt
- Both packages installable and importable
  </done>
</task>

<task type="auto">
  <name>Task 2: Create generic RSS source base class</name>
  <files>app/services/sources/rss_source.py</files>
  <action>
Create `rss_source.py` with a reusable RSS feed source:

```python
"""
Generic RSS feed news source using feedparser and aiohttp.
"""
import logging
from datetime import datetime
from typing import ClassVar
import asyncio

import aiohttp
import feedparser

from app.services.sources.base import NewsSource, ScrapedNewsItem

logger = logging.getLogger(__name__)


class RSSNewsSource(NewsSource):
    """
    Base class for RSS feed news sources.

    Subclasses should set FEED_URLS and SOURCE_NAME.
    """

    SOURCE_NAME: ClassVar[str] = "rss_generic"
    FEED_URLS: ClassVar[list[str]] = []
    TIMEOUT_SECONDS: ClassVar[int] = 30

    async def search(self, query: str, max_results: int = 10) -> list[ScrapedNewsItem]:
        """
        Search RSS feeds for items matching query.

        Fetches all configured feeds, filters by query keywords,
        returns up to max_results items.
        """
        if not self.FEED_URLS:
            logger.warning(f"{self.SOURCE_NAME}: No feed URLs configured")
            return []

        all_items: list[ScrapedNewsItem] = []

        # Fetch all feeds concurrently
        async with aiohttp.ClientSession() as session:
            tasks = [self._fetch_feed(session, url) for url in self.FEED_URLS]
            results = await asyncio.gather(*tasks, return_exceptions=True)

        # Collect items from successful fetches
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"{self.SOURCE_NAME}: Feed fetch error: {result}")
                continue
            all_items.extend(result)

        # Filter by query
        filtered = self._filter_by_query(all_items, query)

        # Sort by date (newest first) and limit
        filtered.sort(key=lambda x: x.published_at or datetime.min, reverse=True)

        return filtered[:max_results]

    async def _fetch_feed(self, session: aiohttp.ClientSession, url: str) -> list[ScrapedNewsItem]:
        """Fetch and parse a single RSS feed."""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=self.TIMEOUT_SECONDS)) as response:
                if response.status != 200:
                    logger.warning(f"{self.SOURCE_NAME}: HTTP {response.status} for {url}")
                    return []

                content = await response.text()

            # feedparser.parse is sync but fast
            feed = feedparser.parse(content)

            if feed.bozo and not feed.entries:
                logger.warning(f"{self.SOURCE_NAME}: Malformed feed at {url}")
                return []

            items = []
            for entry in feed.entries:
                item = self._parse_entry(entry)
                if item:
                    items.append(item)

            return items

        except asyncio.TimeoutError:
            logger.error(f"{self.SOURCE_NAME}: Timeout fetching {url}")
            return []
        except Exception as e:
            logger.error(f"{self.SOURCE_NAME}: Error fetching {url}: {e}")
            return []

    def _parse_entry(self, entry: feedparser.FeedParserDict) -> ScrapedNewsItem | None:
        """Parse a feed entry into ScrapedNewsItem."""
        title = entry.get("title", "").strip()
        if not title:
            return None

        description = entry.get("summary", "") or entry.get("description", "")

        # Parse publication date
        published_at = None
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try:
                published_at = datetime(*entry.published_parsed[:6])
            except (TypeError, ValueError):
                pass
        elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
            try:
                published_at = datetime(*entry.updated_parsed[:6])
            except (TypeError, ValueError):
                pass

        return ScrapedNewsItem(
            title=title,
            description=description[:500] if description else None,  # Limit description length
            url=entry.get("link"),
            source=self.SOURCE_NAME,
            published_at=published_at,
            raw_data=dict(entry),
        )

    def _filter_by_query(self, items: list[ScrapedNewsItem], query: str) -> list[ScrapedNewsItem]:
        """Filter items by query keywords."""
        if not query:
            return items

        # Extract keywords from query (handle quotes and OR)
        # e.g., '"Bradesco Saude" OR "ANS 123456"' -> ['bradesco', 'saude', 'ans', '123456']
        query_lower = query.lower()
        # Remove quotes and OR, split into words
        cleaned = query_lower.replace('"', ' ').replace(' or ', ' ')
        keywords = [w.strip() for w in cleaned.split() if len(w.strip()) > 2]

        if not keywords:
            return items

        filtered = []
        for item in items:
            text = f"{item.title} {item.description or ''}".lower()
            if any(kw in text for kw in keywords):
                filtered.append(item)

        return filtered

    async def health_check(self) -> dict:
        """Check if feeds are accessible."""
        if not self.FEED_URLS:
            return {"status": "error", "message": "No feed URLs configured"}

        try:
            async with aiohttp.ClientSession() as session:
                async with session.head(
                    self.FEED_URLS[0],
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status < 400:
                        return {"status": "ok", "feeds": len(self.FEED_URLS)}
                    return {"status": "error", "message": f"HTTP {response.status}"}
        except Exception as e:
            return {"status": "error", "message": str(e)}
```

Key design decisions:
- Concurrent feed fetching with aiohttp
- feedparser handles edge cases (encoding, malformed XML)
- Keyword filtering works with quoted phrases and OR syntax
- Description truncated to 500 chars to prevent memory issues
- Graceful degradation on errors (return empty list, log warning)
  </action>
  <verify>
```bash
python -c "
from app.services.sources.rss_source import RSSNewsSource
print(f'RSSNewsSource importable: {RSSNewsSource is not None}')
print(f'Is subclass of NewsSource: {hasattr(RSSNewsSource, \"search\")}')
"
```
  </verify>
  <done>
- RSSNewsSource class created with search(), health_check()
- Concurrent feed fetching via aiohttp
- Keyword filtering for query matching
- Date parsing handles multiple formats
- Graceful error handling
  </done>
</task>

<task type="auto">
  <name>Task 3: Create InfoMoney, Estadao, and ANS sources</name>
  <files>
    app/services/sources/infomoney.py
    app/services/sources/estadao.py
    app/services/sources/ans.py
    app/services/sources/__init__.py
  </files>
  <action>
Create three RSS-based sources and update __init__.py:

**infomoney.py:**
```python
"""InfoMoney RSS news source."""
from typing import ClassVar
from app.services.sources.rss_source import RSSNewsSource
from app.services.sources.base import SourceRegistry


class InfoMoneySource(RSSNewsSource):
    """
    InfoMoney news source via RSS feeds.

    Covers: Mercados, Economia, Business sections.
    Relevant for financial/insurance news.
    """

    SOURCE_NAME: ClassVar[str] = "infomoney"
    FEED_URLS: ClassVar[list[str]] = [
        "https://www.infomoney.com.br/mercados/feed/",
        "https://www.infomoney.com.br/economia/feed/",
        "https://www.infomoney.com.br/business/feed/",
    ]


# Auto-register on import
SourceRegistry.register(InfoMoneySource())
```

**estadao.py:**
```python
"""Estadao/Broadcast RSS news source."""
from typing import ClassVar
from app.services.sources.rss_source import RSSNewsSource
from app.services.sources.base import SourceRegistry


class EstadaoSource(RSSNewsSource):
    """
    Estadao news source via RSS feeds.

    Broadcast is part of Estadao group.
    Covers economia section for insurance/financial news.
    """

    SOURCE_NAME: ClassVar[str] = "estadao"
    FEED_URLS: ClassVar[list[str]] = [
        "https://www.estadao.com.br/economia/rss",
        "https://www.estadao.com.br/negocios/rss",
    ]


# Auto-register on import
SourceRegistry.register(EstadaoSource())
```

**ans.py:**
```python
"""ANS (Agencia Nacional de Saude Suplementar) official releases source."""
from typing import ClassVar
from app.services.sources.rss_source import RSSNewsSource
from app.services.sources.base import SourceRegistry


class ANSSource(RSSNewsSource):
    """
    ANS official news source via gov.br RSS.

    Official regulatory releases from Brazil's health
    insurance regulatory agency.
    """

    SOURCE_NAME: ClassVar[str] = "ans"
    FEED_URLS: ClassVar[list[str]] = [
        "https://www.gov.br/ans/pt-br/assuntos/noticias/RSS",
    ]


# Auto-register on import
SourceRegistry.register(ANSSource())
```

**Update __init__.py:**
Add imports to trigger auto-registration:
```python
"""News source abstraction package."""
from app.services.sources.base import NewsSource, SourceRegistry, ScrapedNewsItem
from app.services.sources.google_news import GoogleNewsSource
from app.services.sources.rss_source import RSSNewsSource
from app.services.sources.infomoney import InfoMoneySource
from app.services.sources.estadao import EstadaoSource
from app.services.sources.ans import ANSSource

__all__ = [
    "NewsSource",
    "SourceRegistry",
    "ScrapedNewsItem",
    "GoogleNewsSource",
    "RSSNewsSource",
    "InfoMoneySource",
    "EstadaoSource",
    "ANSSource",
]
```

Each source has:
- Unique SOURCE_NAME for identification
- Appropriate FEED_URLS for the domain
- Auto-registration with SourceRegistry
  </action>
  <verify>
```bash
python -c "
from app.services.sources import SourceRegistry
names = SourceRegistry.names()
print(f'Registered sources: {names}')
print(f'Expected 4 sources: {len(names) == 4}')
for name in ['google_news', 'infomoney', 'estadao', 'ans']:
    src = SourceRegistry.get(name)
    print(f'  {name}: {src is not None}')
"
```
Verify: 4 sources registered (google_news, infomoney, estadao, ans)
  </verify>
  <done>
- InfoMoneySource registered with 3 feed URLs
- EstadaoSource registered with 2 feed URLs
- ANSSource registered with 1 feed URL
- All sources inherit from RSSNewsSource
- SourceRegistry contains all 4 sources
- Package __init__.py exports all classes
  </done>
</task>

</tasks>

<verification>
Full module test:
```bash
# Dependencies installed
pip install feedparser aiohttp

# All sources registered
python -c "
from app.services.sources import SourceRegistry
print(f'Sources: {SourceRegistry.names()}')
assert len(SourceRegistry.names()) >= 4
"

# RSS source can fetch (async test)
python -c "
import asyncio
from app.services.sources import InfoMoneySource

async def test():
    src = InfoMoneySource()
    hc = await src.health_check()
    print(f'InfoMoney health: {hc}')

asyncio.run(test())
"

# Backward compatibility
python -c "
from app.services.scraper import ApifyScraperService
print('scraper.py still works')
"
```
</verification>

<success_criteria>
1. feedparser and aiohttp in requirements.txt and installable
2. RSSNewsSource provides base RSS fetching with keyword filtering
3. InfoMoneySource scrapes 3 InfoMoney feeds (NEWS-03)
4. EstadaoSource scrapes 2 Estadao feeds (NEWS-06)
5. ANSSource scrapes ANS gov.br feed (NEWS-05)
6. All sources auto-register with SourceRegistry
7. health_check() works for each source
</success_criteria>

<output>
After completion, create `.planning/phases/03-news-collection-scale/03-02-SUMMARY.md`
</output>
