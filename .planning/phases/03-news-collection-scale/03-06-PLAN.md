---
phase: 03-news-collection-scale
plan: 06
type: execute
wave: 4
depends_on: ["03-01", "03-02", "03-03", "03-04", "03-05"]
files_modified:
  - app/services/scraper.py
  - app/routers/runs.py
autonomous: true

must_haves:
  truths:
    - "System scrapes all 6 sources for each insurer"
    - "System processes insurers in batches with rate limiting"
    - "System stores news items linked to insurers and runs"
    - "Run endpoint supports full category processing"
  artifacts:
    - path: "app/services/scraper.py"
      provides: "Unified scraper with all sources"
      exports: ["ScraperService"]
    - path: "app/routers/runs.py"
      provides: "Enhanced run endpoint"
      contains: "process_category"
  key_links:
    - from: "app/services/scraper.py"
      to: "app/services/batch_processor.py"
      via: "BatchProcessor import"
      pattern: "from app\\.services\\.batch_processor import BatchProcessor"
    - from: "app/services/scraper.py"
      to: "app/services/relevance_scorer.py"
      via: "RelevanceScorer import"
      pattern: "from app\\.services\\.relevance_scorer import RelevanceScorer"
    - from: "app/routers/runs.py"
      to: "app/services/scraper.py"
      via: "ScraperService usage"
      pattern: "ScraperService"
---

<objective>
Integrate batch processor and relevance scorer into the scraper service and update the run endpoint to support full category processing.

Purpose: This plan wires together all Phase 3 components (6 sources, batch processor, relevance scorer) into a unified scraper service and updates the API to support processing entire categories with 897 insurers.

Output: Updated scraper.py with multi-source support and runs.py with full category processing.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-news-collection-scale/03-RESEARCH.md
@.planning/phases/03-news-collection-scale/03-01-SUMMARY.md
@.planning/phases/03-news-collection-scale/03-02-SUMMARY.md
@.planning/phases/03-news-collection-scale/03-03-SUMMARY.md
@.planning/phases/03-news-collection-scale/03-04-SUMMARY.md
@.planning/phases/03-news-collection-scale/03-05-SUMMARY.md

# Existing code to modify
@app/services/scraper.py
@app/routers/runs.py
@app/services/batch_processor.py
@app/services/relevance_scorer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update scraper service with multi-source support</name>
  <files>app/services/scraper.py</files>
  <action>
Refactor `app/services/scraper.py` to integrate all Phase 3 components:

```python
"""
Unified scraper service for BrasilIntel.

Integrates multiple news sources with batch processing and relevance scoring.
Maintains backward compatibility with Phase 2 ApifyScraperService.
"""
import logging
from typing import Any
from datetime import datetime

from sqlalchemy.orm import Session

from app.config import get_settings
from app.services.sources import (
    SourceRegistry,
    ScrapedNewsItem,
    GoogleNewsSource,
)
from app.services.batch_processor import BatchProcessor, BatchProgress
from app.services.relevance_scorer import RelevanceScorer
from app.models.insurer import Insurer
from app.models.run import Run

logger = logging.getLogger(__name__)

# Re-export for backward compatibility
__all__ = ["ScrapedNewsItem", "ApifyScraperService", "ScraperService"]


class ApifyScraperService:
    """
    Legacy scraper service for backward compatibility.

    Maintained for Phase 2 code that imports ApifyScraperService.
    New code should use ScraperService instead.
    """

    def __init__(self):
        self._google_source = GoogleNewsSource()

    def search_google_news(
        self,
        query: str,
        language: str = "pt",
        country: str = "BR",
        max_results: int = 10,
        time_filter: str = "7d",
        timeout_secs: int = 300,
    ) -> list[ScrapedNewsItem]:
        """Search Google News for a query string."""
        # Note: GoogleNewsSource.search is async but we call it sync here
        # for backward compatibility. The batch processor handles async properly.
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        return loop.run_until_complete(
            self._google_source.search(query, max_results)
        )

    def search_insurer(
        self,
        insurer_name: str,
        ans_code: str,
        max_results: int = 10,
    ) -> list[ScrapedNewsItem]:
        """Search for news about a specific insurer."""
        query = f'"{insurer_name}" OR "ANS {ans_code}"'
        return self.search_google_news(query=query, max_results=max_results)

    def health_check(self) -> dict[str, Any]:
        """Check scraper service connectivity."""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        return loop.run_until_complete(self._google_source.health_check())


class ScraperService:
    """
    Unified scraper service with multi-source support.

    Integrates:
    - 6 news sources (Google News, InfoMoney, Estadao, ANS, Valor, CQCS)
    - Batch processing for 897 insurers
    - Relevance scoring pre-filter

    This is the recommended service for Phase 3+ code.
    """

    def __init__(
        self,
        sources: list[str] | None = None,
        use_relevance_scoring: bool = True,
    ):
        """
        Initialize scraper service.

        Args:
            sources: List of source names to use (default: all registered)
            use_relevance_scoring: Whether to filter by relevance (default: True)
        """
        settings = get_settings()

        # Get sources from registry
        if sources:
            self.sources = [
                SourceRegistry.get(name)
                for name in sources
                if SourceRegistry.get(name)
            ]
        else:
            self.sources = SourceRegistry.get_all()

        # Initialize batch processor with selected sources
        self.batch_processor = BatchProcessor(sources=self.sources)

        # Initialize relevance scorer
        self.use_relevance_scoring = use_relevance_scoring
        self.relevance_scorer = RelevanceScorer() if use_relevance_scoring else None

        logger.info(
            f"ScraperService initialized: {len(self.sources)} sources, "
            f"relevance_scoring={use_relevance_scoring}"
        )

    async def scrape_insurer(
        self,
        insurer: Insurer,
        max_results_per_source: int = 10,
    ) -> list[ScrapedNewsItem]:
        """
        Scrape all sources for a single insurer.

        Args:
            insurer: Insurer object to scrape for
            max_results_per_source: Max results per source

        Returns:
            List of scraped and relevance-filtered items
        """
        all_items: list[ScrapedNewsItem] = []
        query = f'"{insurer.name}" OR "ANS {insurer.ans_code}"'

        # Scrape each source
        for source in self.sources:
            try:
                items = await source.search(query, max_results_per_source)
                all_items.extend(items)
                logger.debug(f"Source {source.SOURCE_NAME}: {len(items)} items")
            except Exception as e:
                logger.warning(f"Source {source.SOURCE_NAME} failed: {e}")

        # Apply relevance scoring if enabled
        if self.relevance_scorer and all_items:
            all_items = self.relevance_scorer.score_batch(
                insurer.name,
                all_items,
                max_results=max_results_per_source * len(self.sources),
            )
            logger.debug(f"After relevance filter: {len(all_items)} items")

        return all_items

    async def process_category(
        self,
        category: str,
        db: Session,
        run: Run | None = None,
        enabled_only: bool = True,
    ) -> BatchProgress:
        """
        Process all insurers in a category.

        Args:
            category: Category name (Health, Dental, Group Life)
            db: Database session
            run: Optional run record for tracking
            enabled_only: Only process enabled insurers

        Returns:
            BatchProgress with statistics
        """
        return await self.batch_processor.process_category(
            category=category,
            db=db,
            run=run,
            enabled_only=enabled_only,
        )

    async def process_insurers(
        self,
        insurers: list[Insurer],
        run: Run | None = None,
        db: Session | None = None,
    ) -> BatchProgress:
        """
        Process a specific list of insurers.

        Args:
            insurers: List of insurers to process
            run: Optional run record
            db: Database session

        Returns:
            BatchProgress with statistics
        """
        return await self.batch_processor.process_insurers(
            insurers=insurers,
            run=run,
            db=db,
        )

    def health_check(self) -> dict[str, Any]:
        """Check all sources and components."""
        import asyncio

        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        async def check_all():
            results = {
                "status": "ok",
                "sources": {},
                "sources_count": len(self.sources),
                "relevance_scoring": self.use_relevance_scoring,
            }

            # Check each source
            errors = 0
            for source in self.sources:
                try:
                    hc = await source.health_check()
                    results["sources"][source.SOURCE_NAME] = hc
                    if hc.get("status") != "ok":
                        errors += 1
                except Exception as e:
                    results["sources"][source.SOURCE_NAME] = {
                        "status": "error",
                        "message": str(e)
                    }
                    errors += 1

            if errors > 0:
                results["status"] = "degraded" if errors < len(self.sources) else "error"
                results["errors"] = errors

            # Check relevance scorer
            if self.relevance_scorer:
                results["relevance_scorer"] = self.relevance_scorer.health_check()

            return results

        return loop.run_until_complete(check_all())

    @property
    def source_names(self) -> list[str]:
        """Get list of configured source names."""
        return [s.SOURCE_NAME for s in self.sources]
```

Key design decisions:
- ApifyScraperService maintained for backward compatibility
- ScraperService is the new recommended entry point
- Integrates batch_processor and relevance_scorer
- Health check reports status of all components
- Async-first design with sync wrappers where needed
  </action>
  <verify>
```bash
python -c "
from app.services.scraper import ApifyScraperService, ScraperService, ScrapedNewsItem

# Legacy service still works
legacy = ApifyScraperService()
print(f'Legacy service: OK')

# New service
service = ScraperService()
print(f'ScraperService sources: {service.source_names}')
print(f'Sources count: {len(service.sources)}')

# Health check
hc = service.health_check()
print(f'Health status: {hc[\"status\"]}')
"
```
Verify: Both services work, ScraperService has 6 sources
  </verify>
  <done>
- ApifyScraperService maintained for backward compatibility
- ScraperService integrates all 6 sources
- Batch processor integrated for category processing
- Relevance scorer integrated for pre-filtering
- Health check reports all component status
- Async methods for proper concurrency
  </done>
</task>

<task type="auto">
  <name>Task 2: Update run endpoint for category processing</name>
  <files>app/routers/runs.py</files>
  <action>
Update `app/routers/runs.py` to support full category processing:

```python
"""
Run orchestration router for BrasilIntel.

Provides endpoints to execute the end-to-end pipeline:
- Single insurer processing (Phase 2 compatibility)
- Full category processing (Phase 3 scale)
"""
import logging
from datetime import datetime
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from app.dependencies import get_db
from app.models.run import Run
from app.models.news_item import NewsItem
from app.models.insurer import Insurer
from app.services.scraper import ApifyScraperService, ScraperService
from app.services.classifier import ClassificationService
from app.services.emailer import GraphEmailService
from app.services.reporter import ReportService
from app.schemas.run import RunRead, RunStatus
from app.schemas.news import NewsItemWithClassification

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/runs", tags=["Runs"])


class ExecuteRequest(BaseModel):
    """Request model for execute endpoint."""
    category: str = Field(description="Category: Health, Dental, or Group Life")
    insurer_id: Optional[int] = Field(None, description="Specific insurer ID (or all in category if None)")
    send_email: bool = Field(True, description="Whether to send email report")
    max_news_items: int = Field(10, ge=1, le=50, description="Max news items per insurer per source")
    process_all: bool = Field(False, description="Process all insurers in category (Phase 3 mode)")


class ExecuteResponse(BaseModel):
    """Response model for execute endpoint."""
    run_id: int
    status: str
    insurers_processed: int
    items_found: int
    email_sent: bool
    message: str
    errors: list[str] = Field(default_factory=list)


class CategoryExecuteRequest(BaseModel):
    """Request model for category execute endpoint."""
    category: str = Field(description="Category: Health, Dental, or Group Life")
    send_email: bool = Field(True, description="Whether to send email report")
    enabled_only: bool = Field(True, description="Only process enabled insurers")


@router.post("/execute", response_model=ExecuteResponse)
async def execute_run(
    request: ExecuteRequest,
    db: Session = Depends(get_db),
) -> ExecuteResponse:
    """
    Execute a scraping run.

    Supports two modes:
    - Single insurer: Provide insurer_id or process_all=False (uses first enabled)
    - Full category: Set process_all=True to process all insurers in category
    """
    # Create run record
    run = Run(
        category=request.category,
        trigger_type="manual",
        status=RunStatus.RUNNING.value,
        started_at=datetime.utcnow(),
    )
    db.add(run)
    db.commit()
    db.refresh(run)

    logger.info(f"Starting run {run.id} for category {request.category}")

    try:
        if request.process_all:
            # Phase 3: Process all insurers in category
            return await _execute_category_run(request, run, db)
        elif request.insurer_id:
            # Specific insurer
            return await _execute_single_insurer_run(request, run, db)
        else:
            # First enabled insurer (Phase 2 behavior)
            return await _execute_single_insurer_run(request, run, db)

    except Exception as e:
        logger.error(f"Run {run.id} failed: {e}")
        run.status = RunStatus.FAILED.value
        run.completed_at = datetime.utcnow()
        run.error_message = str(e)
        db.commit()
        raise HTTPException(status_code=500, detail=str(e))


async def _execute_single_insurer_run(
    request: ExecuteRequest,
    run: Run,
    db: Session,
) -> ExecuteResponse:
    """Execute run for a single insurer (Phase 2 mode)."""
    # Get insurer
    if request.insurer_id:
        insurer = db.query(Insurer).filter(Insurer.id == request.insurer_id).first()
        if not insurer:
            raise HTTPException(status_code=404, detail=f"Insurer {request.insurer_id} not found")
    else:
        insurer = db.query(Insurer).filter(
            Insurer.category == request.category,
            Insurer.enabled == True
        ).first()
        if not insurer:
            raise HTTPException(status_code=404, detail=f"No enabled insurers in {request.category}")

    logger.info(f"Processing insurer: {insurer.name} (ANS: {insurer.ans_code})")

    # Use new ScraperService for single insurer
    scraper = ScraperService()
    classifier = ClassificationService()

    # Scrape all sources
    logger.info(f"Scraping news for {insurer.name}...")
    scraped_items = await scraper.scrape_insurer(
        insurer=insurer,
        max_results_per_source=request.max_news_items,
    )

    logger.info(f"Found {len(scraped_items)} news items from {len(scraper.sources)} sources")

    items_stored = 0

    # Classify and store each news item
    for scraped in scraped_items:
        logger.info(f"Classifying: {scraped.title[:100]}...")

        classification = classifier.classify_single_news(
            insurer_name=insurer.name,
            news_title=scraped.title,
            news_description=scraped.description,
        )

        news_item = NewsItem(
            run_id=run.id,
            insurer_id=insurer.id,
            title=scraped.title,
            description=scraped.description,
            source_url=scraped.url,
            source_name=scraped.source,
            published_at=scraped.published_at,
            status=classification.status if classification else None,
            sentiment=classification.sentiment if classification else None,
            summary="\n".join(classification.summary_bullets) if classification else None,
        )
        db.add(news_item)
        items_stored += 1

    db.commit()
    logger.info(f"Stored {items_stored} news items")

    # Generate and send report
    email_sent = await _generate_and_send_report(
        request.category, run.id, db, request.send_email
    )

    # Update run status
    run.status = RunStatus.COMPLETED.value
    run.completed_at = datetime.utcnow()
    run.insurers_processed = 1
    run.items_found = items_stored
    db.commit()

    return ExecuteResponse(
        run_id=run.id,
        status=run.status,
        insurers_processed=1,
        items_found=items_stored,
        email_sent=email_sent,
        message=f"Successfully processed {insurer.name} with {items_stored} news items",
    )


async def _execute_category_run(
    request: ExecuteRequest,
    run: Run,
    db: Session,
) -> ExecuteResponse:
    """Execute run for entire category (Phase 3 mode)."""
    logger.info(f"Processing all insurers in category {request.category}")

    scraper = ScraperService()
    classifier = ClassificationService()

    # Process all insurers
    progress = await scraper.process_category(
        category=request.category,
        db=db,
        run=run,
        enabled_only=True,
    )

    logger.info(
        f"Batch processing complete: {progress.processed_insurers} insurers, "
        f"{progress.total_items_found} items"
    )

    # Classify stored items (batch classification)
    logger.info("Classifying news items...")
    news_items = db.query(NewsItem).filter(
        NewsItem.run_id == run.id,
        NewsItem.status == None,  # Unclassified items
    ).all()

    for item in news_items:
        insurer = db.query(Insurer).filter(Insurer.id == item.insurer_id).first()
        if not insurer:
            continue

        classification = classifier.classify_single_news(
            insurer_name=insurer.name,
            news_title=item.title,
            news_description=item.description,
        )

        if classification:
            item.status = classification.status
            item.sentiment = classification.sentiment
            item.summary = "\n".join(classification.summary_bullets)

    db.commit()
    logger.info(f"Classified {len(news_items)} items")

    # Generate and send report
    email_sent = await _generate_and_send_report(
        request.category, run.id, db, request.send_email
    )

    # Update run status
    run.status = RunStatus.COMPLETED.value
    run.completed_at = datetime.utcnow()
    db.commit()

    return ExecuteResponse(
        run_id=run.id,
        status=run.status,
        insurers_processed=progress.processed_insurers,
        items_found=progress.total_items_found,
        email_sent=email_sent,
        message=f"Processed {progress.processed_insurers} insurers with {progress.total_items_found} news items",
        errors=progress.errors[:10],  # Limit errors in response
    )


async def _generate_and_send_report(
    category: str,
    run_id: int,
    db: Session,
    send_email: bool,
) -> bool:
    """Generate HTML report and optionally send via email."""
    logger.info("Generating HTML report...")
    report_service = ReportService()
    html_report = report_service.generate_report_from_db(
        category=category,
        run_id=run_id,
        db_session=db,
    )

    email_sent = False
    if send_email:
        logger.info("Sending email report...")
        email_service = GraphEmailService()
        report_date = datetime.now().strftime("%Y-%m-%d")
        email_result = await email_service.send_report_email(
            category=category,
            html_content=html_report,
            report_date=report_date,
        )

        if email_result.get("status") == "ok":
            email_sent = True
            logger.info("Email sent successfully")
        else:
            logger.warning(f"Email not sent: {email_result.get('message')}")

    return email_sent


@router.post("/execute/category", response_model=ExecuteResponse)
async def execute_category_run(
    request: CategoryExecuteRequest,
    db: Session = Depends(get_db),
) -> ExecuteResponse:
    """
    Execute a full category run (Phase 3 dedicated endpoint).

    Processes all insurers in the category using batch processing.
    """
    # Create execute request with process_all=True
    execute_request = ExecuteRequest(
        category=request.category,
        send_email=request.send_email,
        process_all=True,
    )

    # Create run record
    run = Run(
        category=request.category,
        trigger_type="manual",
        status=RunStatus.RUNNING.value,
        started_at=datetime.utcnow(),
    )
    db.add(run)
    db.commit()
    db.refresh(run)

    try:
        return await _execute_category_run(execute_request, run, db)
    except Exception as e:
        run.status = RunStatus.FAILED.value
        run.completed_at = datetime.utcnow()
        run.error_message = str(e)
        db.commit()
        raise HTTPException(status_code=500, detail=str(e))


@router.get("", response_model=list[RunRead])
def list_runs(
    category: Optional[str] = None,
    status: Optional[str] = None,
    limit: int = 20,
    db: Session = Depends(get_db),
) -> list[Run]:
    """List runs with optional filtering."""
    query = db.query(Run)

    if category:
        query = query.filter(Run.category == category)

    if status:
        query = query.filter(Run.status == status)

    runs = query.order_by(Run.started_at.desc()).limit(limit).all()

    return runs


@router.get("/{run_id}", response_model=RunRead)
def get_run(
    run_id: int,
    db: Session = Depends(get_db),
) -> Run:
    """Get a specific run by ID."""
    run = db.query(Run).filter(Run.id == run_id).first()

    if not run:
        raise HTTPException(status_code=404, detail=f"Run {run_id} not found")

    return run


@router.get("/{run_id}/news", response_model=list[NewsItemWithClassification])
def get_run_news(
    run_id: int,
    db: Session = Depends(get_db),
) -> list[NewsItem]:
    """Get news items for a specific run."""
    run = db.query(Run).filter(Run.id == run_id).first()

    if not run:
        raise HTTPException(status_code=404, detail=f"Run {run_id} not found")

    news_items = db.query(NewsItem).filter(NewsItem.run_id == run_id).all()

    return news_items
```

Key changes:
- Added `process_all` flag to ExecuteRequest
- Added dedicated `/execute/category` endpoint
- Single insurer mode now uses ScraperService (all 6 sources)
- Category mode uses batch processor
- Classification happens after scraping (batch workflow)
- Report generation extracted to helper function
  </action>
  <verify>
```bash
python -c "
from app.routers.runs import router, ExecuteRequest, CategoryExecuteRequest

# Check endpoints
routes = [r.path for r in router.routes]
print(f'Routes: {routes}')
assert '/execute' in routes or 'execute' in str(routes)
print('execute endpoint: OK')

# Check request models
req = ExecuteRequest(category='Health', process_all=True)
print(f'ExecuteRequest process_all: {req.process_all}')

cat_req = CategoryExecuteRequest(category='Health')
print(f'CategoryExecuteRequest: {cat_req.category}')
"
```
Verify: Routes include /execute and /execute/category, request models work
  </verify>
  <done>
- ExecuteRequest extended with process_all flag
- Single insurer mode uses ScraperService with all 6 sources
- Category mode uses batch processor for all insurers
- Dedicated /execute/category endpoint for Phase 3
- Classification runs after scraping (batch-friendly)
- Error reporting in response
- Backward compatible with Phase 2 usage
  </done>
</task>

<task type="auto">
  <name>Task 3: Update health endpoint with scraper status</name>
  <files>app/routers/runs.py</files>
  <action>
Add health check endpoint for scraper service to runs.py:

Add the following endpoint after the other run endpoints:

```python
@router.get("/health/scraper", tags=["Health"])
def scraper_health() -> dict:
    """
    Check health of all scraper components.

    Returns status of:
    - All 6 news sources
    - Batch processor configuration
    - Relevance scorer
    """
    scraper = ScraperService()
    return scraper.health_check()
```

This provides visibility into the scraping infrastructure status.
  </action>
  <verify>
```bash
python -c "
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)
response = client.get('/api/runs/health/scraper')
print(f'Status code: {response.status_code}')
if response.status_code == 200:
    data = response.json()
    print(f'Sources count: {data.get(\"sources_count\")}')
    print(f'Status: {data.get(\"status\")}')
"
```
Verify: Health endpoint returns scraper status
  </verify>
  <done>
- /api/runs/health/scraper endpoint added
- Returns status of all 6 sources
- Reports batch processor config
- Reports relevance scorer status
  </done>
</task>

</tasks>

<verification>
Full Phase 3 integration verification:
```bash
# All imports work
python -c "
from app.services.scraper import ScraperService, ApifyScraperService
from app.services.batch_processor import BatchProcessor
from app.services.relevance_scorer import RelevanceScorer
from app.services.sources import SourceRegistry
print('All imports: OK')
"

# ScraperService has all sources
python -c "
from app.services.scraper import ScraperService
s = ScraperService()
print(f'Sources: {s.source_names}')
assert len(s.sources) == 6, f'Expected 6 sources, got {len(s.sources)}'
"

# Run endpoint accessible
python -c "
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)
print('API endpoints:')
for route in app.routes:
    if hasattr(route, 'path') and 'run' in route.path:
        print(f'  {route.path}')
"

# Health check works
python -c "
from app.services.scraper import ScraperService
s = ScraperService()
hc = s.health_check()
print(f'Health: {hc[\"status\"]} ({hc[\"sources_count\"]} sources)')
"
```
</verification>

<success_criteria>
1. ScraperService integrates all 6 sources
2. Batch processor handles category-wide processing
3. Relevance scorer pre-filters items
4. /execute endpoint supports both single and category modes
5. /execute/category dedicated endpoint for Phase 3
6. Health endpoint reports all component status
7. Backward compatibility with Phase 2 code maintained
</success_criteria>

<output>
After completion, create `.planning/phases/03-news-collection-scale/03-06-SUMMARY.md`
</output>
