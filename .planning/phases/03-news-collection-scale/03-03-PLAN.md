---
phase: 03-news-collection-scale
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - app/services/sources/valor.py
  - app/services/sources/cqcs.py
  - app/config.py
autonomous: true

must_haves:
  truths:
    - "System scrapes Valor Economico for insurance news"
    - "System scrapes CQCS for insurance industry news"
    - "Batch processing settings are configurable"
  artifacts:
    - path: "app/services/sources/valor.py"
      provides: "Valor Economico web crawler source"
      exports: ["ValorSource"]
    - path: "app/services/sources/cqcs.py"
      provides: "CQCS web crawler source"
      exports: ["CQCSSource"]
    - path: "app/config.py"
      provides: "Extended configuration"
      contains: "batch_size"
  key_links:
    - from: "app/services/sources/valor.py"
      to: "apify_client"
      via: "Website Content Crawler"
      pattern: "apify/website-content-crawler"
    - from: "app/config.py"
      to: "batch_processor"
      via: "settings import"
      pattern: "batch_size.*30"
---

<objective>
Implement website crawler sources for Valor Economico and CQCS using Apify Website Content Crawler, and extend configuration for batch processing settings.

Purpose: Complete the 6 news sources with the two that lack RSS feeds (NEWS-02 Valor, NEWS-04 CQCS). Add configuration for batch processing (NEWS-07 settings). Both sites require website crawling as they don't expose RSS.

Output: Two Apify-based crawler sources and extended config.py with batch processing settings.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-news-collection-scale/03-RESEARCH.md
@.planning/phases/03-news-collection-scale/03-01-SUMMARY.md

# Existing code
@app/services/sources/base.py
@app/services/sources/google_news.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend configuration for batch processing</name>
  <files>app/config.py</files>
  <action>
Add batch processing configuration to Settings class:

```python
# Batch Processing
batch_size: int = 30  # Insurers per batch (NEWS-07: 30-50)
batch_delay_seconds: float = 2.0  # Delay between batches
max_concurrent_sources: int = 3  # Max sources to query in parallel
scrape_timeout_seconds: int = 60  # Per-source timeout
scrape_max_results: int = 10  # Max results per insurer per source

# Source-specific settings (can override defaults)
source_timeout_valor: int = 120  # Valor needs longer timeout
source_timeout_cqcs: int = 120  # CQCS needs longer timeout
```

Place after the "Application" section, before "Report settings".

Also add a helper method:
```python
def get_source_timeout(self, source_name: str) -> int:
    """Get timeout for a specific source, with fallback to default."""
    specific = getattr(self, f"source_timeout_{source_name}", None)
    return specific if specific else self.scrape_timeout_seconds
```

These settings allow tuning based on research findings (30-50 batch size optimal, 2s delay prevents rate limiting).
  </action>
  <verify>
```bash
python -c "
from app.config import get_settings
s = get_settings()
print(f'batch_size: {s.batch_size}')
print(f'batch_delay: {s.batch_delay_seconds}')
print(f'max_concurrent: {s.max_concurrent_sources}')
print(f'valor timeout: {s.get_source_timeout(\"valor\")}')
print(f'default timeout: {s.get_source_timeout(\"unknown\")}')
"
```
Verify: batch_size=30, batch_delay=2.0, valor timeout=120, unknown timeout=60
  </verify>
  <done>
- batch_size configurable (default 30)
- batch_delay_seconds configurable (default 2.0)
- max_concurrent_sources configurable (default 3)
- Source-specific timeouts supported
- get_source_timeout() helper method works
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Valor Economico website crawler source</name>
  <files>app/services/sources/valor.py</files>
  <action>
Create Valor Economico source using Apify Website Content Crawler:

```python
"""
Valor Economico website crawler news source.

Uses Apify Website Content Crawler since Valor has no RSS feed.
Searches valor.globo.com for insurance-related news.
"""
import logging
from typing import ClassVar, Any
from datetime import datetime
from urllib.parse import quote

from apify_client import ApifyClient

from app.config import get_settings
from app.services.sources.base import NewsSource, SourceRegistry, ScrapedNewsItem

logger = logging.getLogger(__name__)


class ValorSource(NewsSource):
    """
    Valor Economico news source via Apify Website Content Crawler.

    Valor is a major Brazilian financial news outlet (part of Globo group).
    No RSS available, so we crawl search results.
    """

    SOURCE_NAME: ClassVar[str] = "valor"
    ACTOR_ID: ClassVar[str] = "apify/website-content-crawler"
    BASE_URL: ClassVar[str] = "https://valor.globo.com"
    SEARCH_PATH: ClassVar[str] = "/busca"

    def __init__(self):
        settings = get_settings()
        if settings.is_apify_configured():
            self.client = ApifyClient(settings.apify_token)
        else:
            self.client = None
        self.timeout = settings.get_source_timeout("valor")

    async def search(self, query: str, max_results: int = 10) -> list[ScrapedNewsItem]:
        """
        Search Valor Economico for news matching query.

        Uses Website Content Crawler to scrape search results page.
        """
        if not self.client:
            logger.warning(f"{self.SOURCE_NAME}: Apify not configured")
            return []

        # Build search URL
        search_url = f"{self.BASE_URL}{self.SEARCH_PATH}?q={quote(query)}"

        run_input = {
            "startUrls": [{"url": search_url}],
            "maxCrawlPages": max_results + 5,  # Over-fetch to account for non-article pages
            "crawlerType": "cheerio",  # Faster than browser for this site
            "maxConcurrency": 3,
            "includeUrlGlobs": [f"{self.BASE_URL}/*"],
            "excludeUrlGlobs": [
                "*/video/*",
                "*/podcast/*",
                "*/autor/*",
            ],
        }

        try:
            logger.info(f"{self.SOURCE_NAME}: Searching for '{query[:50]}...'")

            run = self.client.actor(self.ACTOR_ID).call(
                run_input=run_input,
                timeout_secs=self.timeout,
            )

            items = list(self.client.dataset(run["defaultDatasetId"]).iterate_items())
            logger.info(f"{self.SOURCE_NAME}: Crawler returned {len(items)} pages")

            return self._parse_results(items, max_results)

        except Exception as e:
            logger.error(f"{self.SOURCE_NAME}: Crawl failed: {e}")
            return []

    def _parse_results(self, items: list[dict[str, Any]], max_results: int) -> list[ScrapedNewsItem]:
        """Parse crawler results into ScrapedNewsItem objects."""
        results = []

        for item in items:
            # Skip non-article pages
            url = item.get("url", "")
            if "/busca" in url or not url.startswith(self.BASE_URL):
                continue

            title = item.get("title", "").strip()
            if not title or len(title) < 10:
                continue

            # Extract text content as description
            text = item.get("text", "") or item.get("description", "")
            description = text[:500] if text else None

            # Try to extract date from URL or metadata
            published_at = self._extract_date(item)

            results.append(ScrapedNewsItem(
                title=title,
                description=description,
                url=url,
                source=self.SOURCE_NAME,
                published_at=published_at,
                raw_data=item,
            ))

            if len(results) >= max_results:
                break

        return results

    def _extract_date(self, item: dict[str, Any]) -> datetime | None:
        """Try to extract publication date from item."""
        # Check metadata first
        if "publishedTime" in item:
            try:
                return datetime.fromisoformat(item["publishedTime"].replace("Z", "+00:00"))
            except (ValueError, TypeError):
                pass

        # Check URL for date pattern (e.g., /2026/02/04/...)
        url = item.get("url", "")
        import re
        match = re.search(r"/(\d{4})/(\d{2})/(\d{2})/", url)
        if match:
            try:
                return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))
            except ValueError:
                pass

        return None

    async def health_check(self) -> dict:
        """Check if Valor source is operational."""
        if not self.client:
            return {"status": "error", "message": "Apify not configured"}

        try:
            user = self.client.user().get()
            return {"status": "ok", "actor": self.ACTOR_ID}
        except Exception as e:
            return {"status": "error", "message": str(e)}


# Auto-register on import
SourceRegistry.register(ValorSource())
```

Key design decisions:
- Uses cheerio crawler (faster than browser)
- Filters out non-article pages (video, podcast, author pages)
- Extracts dates from URL patterns and metadata
- Over-fetches then filters to max_results
- Configurable timeout via settings
  </action>
  <verify>
```bash
python -c "
from app.services.sources.valor import ValorSource
from app.services.sources import SourceRegistry

src = ValorSource()
print(f'SOURCE_NAME: {src.SOURCE_NAME}')
print(f'Timeout: {src.timeout}')
print(f'Registered: {SourceRegistry.get(\"valor\") is not None}')
"
```
Verify: SOURCE_NAME is "valor", timeout is 120, source registered
  </verify>
  <done>
- ValorSource implements NewsSource
- Uses Website Content Crawler actor
- Filters search results to articles only
- Extracts dates from URL and metadata
- Auto-registers with SourceRegistry
- Uses configurable timeout (120s)
  </done>
</task>

<task type="auto">
  <name>Task 3: Create CQCS website crawler source</name>
  <files>
    app/services/sources/cqcs.py
    app/services/sources/__init__.py
  </files>
  <action>
Create CQCS source and update package exports:

**cqcs.py:**
```python
"""
CQCS (Conselho Qualificado de Corretores de Seguros) website crawler source.

Uses Apify Website Content Crawler. CQCS is a major Brazilian
insurance industry news source with no RSS feed.
"""
import logging
from typing import ClassVar, Any
from datetime import datetime
from urllib.parse import quote
import re

from apify_client import ApifyClient

from app.config import get_settings
from app.services.sources.base import NewsSource, SourceRegistry, ScrapedNewsItem

logger = logging.getLogger(__name__)


class CQCSSource(NewsSource):
    """
    CQCS news source via Apify Website Content Crawler.

    CQCS is a primary source for Brazilian insurance industry news.
    No RSS available, may have anti-bot protection.
    """

    SOURCE_NAME: ClassVar[str] = "cqcs"
    ACTOR_ID: ClassVar[str] = "apify/website-content-crawler"
    BASE_URL: ClassVar[str] = "https://cqcs.com.br"
    NEWS_PATH: ClassVar[str] = "/noticias"

    def __init__(self):
        settings = get_settings()
        if settings.is_apify_configured():
            self.client = ApifyClient(settings.apify_token)
        else:
            self.client = None
        self.timeout = settings.get_source_timeout("cqcs")

    async def search(self, query: str, max_results: int = 10) -> list[ScrapedNewsItem]:
        """
        Search CQCS for news matching query.

        CQCS may have anti-bot protection, so we use browser-based
        crawling and crawl the news section looking for matches.
        """
        if not self.client:
            logger.warning(f"{self.SOURCE_NAME}: Apify not configured")
            return []

        # CQCS search might be limited, so we crawl news section
        # and filter by query keywords
        start_url = f"{self.BASE_URL}{self.NEWS_PATH}"

        # Use browser crawler for CQCS (more robust against anti-bot)
        run_input = {
            "startUrls": [{"url": start_url}],
            "maxCrawlPages": max_results * 3,  # Over-fetch for filtering
            "crawlerType": "playwright:firefox",  # Browser-based for anti-bot
            "maxConcurrency": 2,  # Conservative concurrency
            "includeUrlGlobs": [f"{self.BASE_URL}/noticias/*"],
            "excludeUrlGlobs": [
                "*/categoria/*",
                "*/autor/*",
                "*/tag/*",
            ],
            "maxRequestRetries": 3,
        }

        try:
            logger.info(f"{self.SOURCE_NAME}: Crawling news section for '{query[:50]}...'")

            run = self.client.actor(self.ACTOR_ID).call(
                run_input=run_input,
                timeout_secs=self.timeout,
            )

            items = list(self.client.dataset(run["defaultDatasetId"]).iterate_items())
            logger.info(f"{self.SOURCE_NAME}: Crawler returned {len(items)} pages")

            # Filter by query keywords
            filtered = self._filter_and_parse(items, query, max_results)
            return filtered

        except Exception as e:
            logger.error(f"{self.SOURCE_NAME}: Crawl failed: {e}")
            return []

    def _filter_and_parse(
        self,
        items: list[dict[str, Any]],
        query: str,
        max_results: int,
    ) -> list[ScrapedNewsItem]:
        """Filter items by query and parse into ScrapedNewsItem objects."""
        # Extract keywords from query
        query_lower = query.lower().replace('"', ' ').replace(' or ', ' ')
        keywords = [w.strip() for w in query_lower.split() if len(w.strip()) > 2]

        results = []

        for item in items:
            url = item.get("url", "")
            if not url.startswith(self.BASE_URL + "/noticias/"):
                continue

            title = item.get("title", "").strip()
            if not title:
                continue

            text = item.get("text", "") or item.get("description", "")
            search_text = f"{title} {text}".lower()

            # Check if any keyword matches
            if keywords and not any(kw in search_text for kw in keywords):
                continue

            description = text[:500] if text else None
            published_at = self._extract_date(item)

            results.append(ScrapedNewsItem(
                title=title,
                description=description,
                url=url,
                source=self.SOURCE_NAME,
                published_at=published_at,
                raw_data=item,
            ))

            if len(results) >= max_results:
                break

        return results

    def _extract_date(self, item: dict[str, Any]) -> datetime | None:
        """Try to extract publication date from item."""
        # Check for common date fields
        for field in ["publishedTime", "datePublished", "date"]:
            if field in item:
                try:
                    return datetime.fromisoformat(str(item[field]).replace("Z", "+00:00"))
                except (ValueError, TypeError):
                    pass

        # Try URL date pattern
        url = item.get("url", "")
        match = re.search(r"/(\d{4})/(\d{2})/(\d{2})/", url)
        if match:
            try:
                return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))
            except ValueError:
                pass

        return None

    async def health_check(self) -> dict:
        """Check if CQCS source is operational."""
        if not self.client:
            return {"status": "error", "message": "Apify not configured"}

        try:
            user = self.client.user().get()
            # Note: CQCS may return 403 on direct access, but Apify can usually get through
            return {"status": "ok", "actor": self.ACTOR_ID, "note": "Uses browser crawler"}
        except Exception as e:
            return {"status": "error", "message": str(e)}


# Auto-register on import
SourceRegistry.register(CQCSSource())
```

**Update __init__.py** to include Valor and CQCS:
```python
"""News source abstraction package."""
from app.services.sources.base import NewsSource, SourceRegistry, ScrapedNewsItem
from app.services.sources.google_news import GoogleNewsSource
from app.services.sources.rss_source import RSSNewsSource
from app.services.sources.infomoney import InfoMoneySource
from app.services.sources.estadao import EstadaoSource
from app.services.sources.ans import ANSSource
from app.services.sources.valor import ValorSource
from app.services.sources.cqcs import CQCSSource

__all__ = [
    "NewsSource",
    "SourceRegistry",
    "ScrapedNewsItem",
    "GoogleNewsSource",
    "RSSNewsSource",
    "InfoMoneySource",
    "EstadaoSource",
    "ANSSource",
    "ValorSource",
    "CQCSSource",
]
```

Key design decisions for CQCS:
- Uses playwright:firefox crawler (browser-based) to handle potential anti-bot
- Lower concurrency (2) to avoid rate limiting
- Crawls news section and filters by keywords
- 3 retries for resilience
  </action>
  <verify>
```bash
python -c "
from app.services.sources import SourceRegistry

names = SourceRegistry.names()
print(f'All sources: {names}')
print(f'Total: {len(names)}')

# Check all 6 sources
expected = ['google_news', 'infomoney', 'estadao', 'ans', 'valor', 'cqcs']
for name in expected:
    src = SourceRegistry.get(name)
    print(f'  {name}: {\"OK\" if src else \"MISSING\"}')
"
```
Verify: 6 sources registered, all expected sources present
  </verify>
  <done>
- CQCSSource implements NewsSource
- Uses browser-based crawler for anti-bot protection
- Filters news section by query keywords
- Auto-registers with SourceRegistry
- Package exports all 6 sources
- All NEWS-02 through NEWS-06 requirements covered
  </done>
</task>

</tasks>

<verification>
Full verification of all sources:
```bash
# All 6 sources registered
python -c "
from app.services.sources import SourceRegistry
names = sorted(SourceRegistry.names())
print(f'Sources ({len(names)}): {names}')
assert len(names) == 6, 'Expected 6 sources'
"

# Config has batch settings
python -c "
from app.config import get_settings
s = get_settings()
print(f'batch_size: {s.batch_size}')
print(f'batch_delay: {s.batch_delay_seconds}')
assert s.batch_size == 30
"

# Crawler sources use correct actor
python -c "
from app.services.sources import ValorSource, CQCSSource
v = ValorSource()
c = CQCSSource()
print(f'Valor actor: {v.ACTOR_ID}')
print(f'CQCS actor: {c.ACTOR_ID}')
assert 'website-content-crawler' in v.ACTOR_ID
"
```
</verification>

<success_criteria>
1. Settings extended with batch_size, batch_delay_seconds, max_concurrent_sources
2. Source-specific timeouts configurable (valor=120, cqcs=120)
3. ValorSource uses Website Content Crawler with cheerio (NEWS-02)
4. CQCSSource uses Website Content Crawler with playwright:firefox (NEWS-04)
5. All 6 sources registered: google_news, infomoney, estadao, ans, valor, cqcs
6. Package __init__.py exports all source classes
</success_criteria>

<output>
After completion, create `.planning/phases/03-news-collection-scale/03-03-SUMMARY.md`
</output>
