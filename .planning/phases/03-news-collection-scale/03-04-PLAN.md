---
phase: 03-news-collection-scale
plan: 04
type: execute
wave: 3
depends_on: ["03-01", "03-02", "03-03"]
files_modified:
  - app/services/batch_processor.py
autonomous: true

must_haves:
  truths:
    - "System processes 897 insurers in configurable batches"
    - "System uses rate limiting between batches"
    - "System tracks progress during batch processing"
    - "Errors in one insurer do not stop the entire batch"
  artifacts:
    - path: "app/services/batch_processor.py"
      provides: "Batch processing orchestration"
      exports: ["BatchProcessor"]
      min_lines: 100
  key_links:
    - from: "app/services/batch_processor.py"
      to: "app/services/sources"
      via: "SourceRegistry import"
      pattern: "from app\\.services\\.sources import SourceRegistry"
    - from: "app/services/batch_processor.py"
      to: "asyncio.Semaphore"
      via: "concurrency control"
      pattern: "asyncio\\.Semaphore"
---

<objective>
Implement batch processing for 897 insurers with semaphore-based rate limiting and progress tracking.

Purpose: Enable scaling from single-insurer scraping to full category processing (NEWS-07). The batch processor orchestrates scraping across all sources for all insurers in a category while respecting rate limits.

Output: BatchProcessor service that processes insurers in configurable batches with concurrency control.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-news-collection-scale/03-RESEARCH.md
@.planning/phases/03-news-collection-scale/03-01-SUMMARY.md
@.planning/phases/03-news-collection-scale/03-02-SUMMARY.md
@.planning/phases/03-news-collection-scale/03-03-SUMMARY.md

# Source abstraction and config
@app/services/sources/base.py
@app/config.py
@app/models/insurer.py
@app/models/run.py
@app/models/news_item.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batch processor service</name>
  <files>app/services/batch_processor.py</files>
  <action>
Create `app/services/batch_processor.py`:

```python
"""
Batch processor for scraping news across all sources for multiple insurers.

Implements semaphore-based rate limiting and progress tracking
to handle 897 insurers without overwhelming news sources.
"""
import asyncio
import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import Callable, Awaitable

from sqlalchemy.orm import Session

from app.config import get_settings
from app.models.insurer import Insurer
from app.models.run import Run
from app.models.news_item import NewsItem
from app.services.sources import SourceRegistry, ScrapedNewsItem, NewsSource

logger = logging.getLogger(__name__)


@dataclass
class BatchProgress:
    """Tracks progress of a batch processing run."""
    total_insurers: int = 0
    processed_insurers: int = 0
    total_items_found: int = 0
    errors: list[str] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.utcnow)

    @property
    def percent_complete(self) -> float:
        if self.total_insurers == 0:
            return 0.0
        return (self.processed_insurers / self.total_insurers) * 100

    @property
    def elapsed_seconds(self) -> float:
        return (datetime.utcnow() - self.start_time).total_seconds()


@dataclass
class InsurerResult:
    """Result of scraping a single insurer."""
    insurer_id: int
    insurer_name: str
    items: list[ScrapedNewsItem] = field(default_factory=list)
    error: str | None = None

    @property
    def success(self) -> bool:
        return self.error is None


class BatchProcessor:
    """
    Orchestrates batch scraping across all sources for multiple insurers.

    Features:
    - Configurable batch size (default 30, NEWS-07 requires 30-50)
    - Semaphore-based concurrency limiting
    - Delay between batches to prevent rate limiting
    - Progress tracking with callback support
    - Graceful error handling (one failure doesn't stop batch)
    """

    def __init__(
        self,
        batch_size: int | None = None,
        max_concurrent: int | None = None,
        delay_seconds: float | None = None,
        sources: list[NewsSource] | None = None,
    ):
        settings = get_settings()

        self.batch_size = batch_size or settings.batch_size
        self.max_concurrent = max_concurrent or settings.max_concurrent_sources
        self.delay_seconds = delay_seconds or settings.batch_delay_seconds
        self.max_results_per_source = settings.scrape_max_results

        # Use provided sources or all registered sources
        self.sources = sources or SourceRegistry.get_all()

        logger.info(
            f"BatchProcessor initialized: batch_size={self.batch_size}, "
            f"max_concurrent={self.max_concurrent}, sources={len(self.sources)}"
        )

    async def process_insurers(
        self,
        insurers: list[Insurer],
        run: Run | None = None,
        db: Session | None = None,
        progress_callback: Callable[[BatchProgress], Awaitable[None]] | None = None,
    ) -> BatchProgress:
        """
        Process a list of insurers, scraping all sources for each.

        Args:
            insurers: List of Insurer objects to process
            run: Optional Run record to update progress
            db: Database session (required if run provided)
            progress_callback: Optional async callback for progress updates

        Returns:
            BatchProgress with final statistics
        """
        progress = BatchProgress(total_insurers=len(insurers))

        if not insurers:
            logger.warning("No insurers to process")
            return progress

        if not self.sources:
            logger.error("No sources configured")
            progress.errors.append("No news sources configured")
            return progress

        logger.info(
            f"Starting batch processing: {len(insurers)} insurers, "
            f"{len(self.sources)} sources"
        )

        # Process in batches
        for batch_start in range(0, len(insurers), self.batch_size):
            batch_end = min(batch_start + self.batch_size, len(insurers))
            batch = insurers[batch_start:batch_end]

            batch_num = (batch_start // self.batch_size) + 1
            total_batches = (len(insurers) + self.batch_size - 1) // self.batch_size

            logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} insurers)")

            # Process batch with concurrency limiting
            results = await self._process_batch(batch)

            # Update progress
            for result in results:
                progress.processed_insurers += 1
                progress.total_items_found += len(result.items)

                if result.error:
                    progress.errors.append(f"{result.insurer_name}: {result.error}")
                    logger.warning(f"Error for {result.insurer_name}: {result.error}")

                # Store items if db provided
                if db and run:
                    await self._store_items(result, run, db)

            # Update run record if provided
            if run and db:
                run.insurers_processed = progress.processed_insurers
                run.items_found = progress.total_items_found
                db.commit()

            # Call progress callback if provided
            if progress_callback:
                await progress_callback(progress)

            # Delay between batches (except for last batch)
            if batch_end < len(insurers):
                logger.debug(f"Waiting {self.delay_seconds}s before next batch")
                await asyncio.sleep(self.delay_seconds)

        logger.info(
            f"Batch processing complete: {progress.processed_insurers} insurers, "
            f"{progress.total_items_found} items, {len(progress.errors)} errors"
        )

        return progress

    async def _process_batch(self, insurers: list[Insurer]) -> list[InsurerResult]:
        """Process a single batch of insurers with concurrency limiting."""
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def process_with_limit(insurer: Insurer) -> InsurerResult:
            async with semaphore:
                return await self._scrape_insurer(insurer)

        results = await asyncio.gather(
            *[process_with_limit(ins) for ins in insurers],
            return_exceptions=True,
        )

        # Convert exceptions to InsurerResult
        final_results = []
        for insurer, result in zip(insurers, results):
            if isinstance(result, Exception):
                final_results.append(InsurerResult(
                    insurer_id=insurer.id,
                    insurer_name=insurer.name,
                    error=str(result),
                ))
            else:
                final_results.append(result)

        return final_results

    async def _scrape_insurer(self, insurer: Insurer) -> InsurerResult:
        """Scrape all sources for a single insurer."""
        result = InsurerResult(
            insurer_id=insurer.id,
            insurer_name=insurer.name,
        )

        # Build search query
        query = f'"{insurer.name}" OR "ANS {insurer.ans_code}"'

        # Scrape each source
        for source in self.sources:
            try:
                items = await source.search(
                    query=query,
                    max_results=self.max_results_per_source,
                )
                result.items.extend(items)

            except Exception as e:
                logger.warning(
                    f"Source {source.SOURCE_NAME} failed for {insurer.name}: {e}"
                )
                # Continue with other sources

        logger.debug(f"Scraped {len(result.items)} items for {insurer.name}")
        return result

    async def _store_items(
        self,
        result: InsurerResult,
        run: Run,
        db: Session,
    ) -> None:
        """Store scraped items in database."""
        for item in result.items:
            news_item = NewsItem(
                run_id=run.id,
                insurer_id=result.insurer_id,
                title=item.title,
                description=item.description,
                source_url=item.url,
                source_name=item.source,
                published_at=item.published_at,
                # Classification fields left null - to be filled by classifier
            )
            db.add(news_item)

        # Commit in batches to avoid lock contention
        if result.items:
            db.flush()

    async def process_category(
        self,
        category: str,
        db: Session,
        run: Run | None = None,
        enabled_only: bool = True,
    ) -> BatchProgress:
        """
        Convenience method to process all insurers in a category.

        Args:
            category: Category name (Health, Dental, Group Life)
            db: Database session
            run: Optional run record
            enabled_only: Only process enabled insurers (default True)

        Returns:
            BatchProgress with final statistics
        """
        query = db.query(Insurer).filter(Insurer.category == category)

        if enabled_only:
            query = query.filter(Insurer.enabled == True)

        insurers = query.all()

        logger.info(f"Processing category {category}: {len(insurers)} insurers")

        return await self.process_insurers(
            insurers=insurers,
            run=run,
            db=db,
        )
```

Key design decisions:
- Semaphore limits concurrent scraping (prevents overwhelming sources)
- Batch + delay pattern for rate limiting
- Progress tracking with callback support
- Graceful error handling (one insurer failure doesn't stop batch)
- Optional Run/DB integration for progress persistence
- Items stored without classification (separate step)
  </action>
  <verify>
```bash
python -c "
from app.services.batch_processor import BatchProcessor, BatchProgress, InsurerResult

bp = BatchProcessor()
print(f'batch_size: {bp.batch_size}')
print(f'max_concurrent: {bp.max_concurrent}')
print(f'sources: {len(bp.sources)}')

# Test progress dataclass
prog = BatchProgress(total_insurers=100, processed_insurers=50, total_items_found=250)
print(f'percent: {prog.percent_complete}%')
"
```
Verify: batch_size=30, max_concurrent=3, 6 sources, percent=50%
  </verify>
  <done>
- BatchProcessor class with process_insurers() and process_category()
- Semaphore-based concurrency control
- Configurable batch_size, max_concurrent, delay_seconds
- BatchProgress dataclass tracks statistics
- InsurerResult captures per-insurer results
- Graceful error handling (continue on failure)
- Optional Run/DB integration for persistence
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for batch processor</name>
  <files>tests/test_batch_processor.py</files>
  <action>
Create basic unit tests for BatchProcessor:

```python
"""Tests for batch processor service."""
import asyncio
import pytest
from unittest.mock import Mock, AsyncMock, patch
from datetime import datetime

from app.services.batch_processor import BatchProcessor, BatchProgress, InsurerResult


class TestBatchProgress:
    """Tests for BatchProgress dataclass."""

    def test_percent_complete_zero_insurers(self):
        """Percent complete should be 0 when no insurers."""
        progress = BatchProgress(total_insurers=0)
        assert progress.percent_complete == 0.0

    def test_percent_complete_partial(self):
        """Percent complete should calculate correctly."""
        progress = BatchProgress(total_insurers=100, processed_insurers=25)
        assert progress.percent_complete == 25.0

    def test_percent_complete_full(self):
        """Percent complete should be 100 when all processed."""
        progress = BatchProgress(total_insurers=50, processed_insurers=50)
        assert progress.percent_complete == 100.0

    def test_elapsed_seconds(self):
        """Elapsed seconds should increase over time."""
        progress = BatchProgress()
        import time
        time.sleep(0.1)
        assert progress.elapsed_seconds >= 0.1


class TestInsurerResult:
    """Tests for InsurerResult dataclass."""

    def test_success_with_items(self):
        """Success should be True when no error."""
        result = InsurerResult(insurer_id=1, insurer_name="Test")
        assert result.success is True

    def test_success_with_error(self):
        """Success should be False when error present."""
        result = InsurerResult(insurer_id=1, insurer_name="Test", error="Failed")
        assert result.success is False


class TestBatchProcessor:
    """Tests for BatchProcessor class."""

    def test_init_defaults(self):
        """Should initialize with default settings."""
        with patch('app.services.batch_processor.SourceRegistry') as mock_registry:
            mock_registry.get_all.return_value = []
            bp = BatchProcessor()
            assert bp.batch_size == 30
            assert bp.max_concurrent == 3
            assert bp.delay_seconds == 2.0

    def test_init_custom_values(self):
        """Should accept custom configuration."""
        bp = BatchProcessor(
            batch_size=50,
            max_concurrent=5,
            delay_seconds=1.0,
            sources=[],
        )
        assert bp.batch_size == 50
        assert bp.max_concurrent == 5
        assert bp.delay_seconds == 1.0

    @pytest.mark.asyncio
    async def test_process_empty_list(self):
        """Should handle empty insurer list gracefully."""
        bp = BatchProcessor(sources=[])
        progress = await bp.process_insurers([])
        assert progress.total_insurers == 0
        assert progress.processed_insurers == 0

    @pytest.mark.asyncio
    async def test_process_no_sources(self):
        """Should report error when no sources configured."""
        bp = BatchProcessor(sources=[])

        mock_insurer = Mock()
        mock_insurer.id = 1
        mock_insurer.name = "Test Insurer"
        mock_insurer.ans_code = "123456"

        progress = await bp.process_insurers([mock_insurer])
        assert len(progress.errors) > 0
        assert "No news sources" in progress.errors[0]


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

Tests verify:
- Progress calculation
- Error detection
- Default configuration
- Empty list handling
- No sources error handling
  </action>
  <verify>
```bash
# Run tests (if pytest installed)
python -m pytest tests/test_batch_processor.py -v 2>/dev/null || python -c "
# Fallback: run basic import tests
from app.services.batch_processor import BatchProcessor, BatchProgress, InsurerResult
print('BatchProcessor imports: OK')

# Test progress
p = BatchProgress(total_insurers=100, processed_insurers=50)
assert p.percent_complete == 50.0, 'Progress calculation failed'
print('BatchProgress: OK')

# Test result
r = InsurerResult(insurer_id=1, insurer_name='Test')
assert r.success is True, 'Success detection failed'
r2 = InsurerResult(insurer_id=2, insurer_name='Test2', error='fail')
assert r2.success is False, 'Error detection failed'
print('InsurerResult: OK')

print('All basic tests passed')
"
```
  </verify>
  <done>
- Test file created at tests/test_batch_processor.py
- Tests cover BatchProgress, InsurerResult, BatchProcessor
- Tests verify initialization, progress tracking, error handling
- Tests can run with pytest or as standalone script
  </done>
</task>

</tasks>

<verification>
Full verification:
```bash
# Module imports
python -c "
from app.services.batch_processor import BatchProcessor, BatchProgress, InsurerResult
print('Imports: OK')
"

# Configuration integration
python -c "
from app.services.batch_processor import BatchProcessor
bp = BatchProcessor()
print(f'Uses settings: batch_size={bp.batch_size}, sources={len(bp.sources)}')
assert bp.batch_size == 30
assert len(bp.sources) == 6
"

# Progress tracking
python -c "
from app.services.batch_processor import BatchProgress
p = BatchProgress(total_insurers=100, processed_insurers=75, total_items_found=500)
print(f'Progress: {p.percent_complete}% complete, {p.total_items_found} items')
"

# Async functionality (basic test)
python -c "
import asyncio
from app.services.batch_processor import BatchProcessor

async def test():
    bp = BatchProcessor(sources=[])
    result = await bp.process_insurers([])
    print(f'Empty process: {result.processed_insurers} processed')

asyncio.run(test())
"
```
</verification>

<success_criteria>
1. BatchProcessor initializes with settings from config.py
2. process_insurers() handles list of Insurer objects
3. Semaphore limits concurrency to max_concurrent
4. Delay applied between batches
5. BatchProgress tracks statistics accurately
6. Errors captured without stopping batch
7. Optional Run/DB integration for persistence
8. Basic tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-news-collection-scale/03-04-SUMMARY.md`
</output>
