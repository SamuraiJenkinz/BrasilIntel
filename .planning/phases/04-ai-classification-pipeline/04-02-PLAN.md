---
phase: 04-ai-classification-pipeline
plan: 02
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - tests/test_classifier.py
  - scripts/migrate_004_category_indicators.py
autonomous: true

must_haves:
  truths:
    - "Integration tests verify classification returns valid status, sentiment, and category_indicators"
    - "Integration tests verify USE_LLM_SUMMARY=false triggers fallback classification"
    - "Database migration adds category_indicators column to existing news_items table"
  artifacts:
    - path: "tests/test_classifier.py"
      provides: "Comprehensive classification service tests"
      min_lines: 100
    - path: "scripts/migrate_004_category_indicators.py"
      provides: "Standalone database migration for category_indicators column"
      contains: "category_indicators"
  key_links:
    - from: "tests/test_classifier.py"
      to: "app/services/classifier.py"
      via: "Tests ClassificationService methods"
      pattern: "ClassificationService"
---

<objective>
Create database migration for category_indicators and comprehensive classification tests.

Purpose: Ensure the classification pipeline works end-to-end with proper test coverage. The database migration allows existing installations to add the new column without data loss.

Output: Standalone migration script and test_classifier.py with unit and integration tests.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-ai-classification-pipeline/04-RESEARCH.md

# Dependencies from Plan 01
@app/schemas/classification.py
@app/services/classifier.py
@app/models/news_item.py

# Test patterns from existing tests
@tests/test_batch_processor.py
@tests/test_relevance_scorer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create standalone database migration for category_indicators column</name>
  <files>scripts/migrate_004_category_indicators.py</files>
  <action>
Create a standalone migration script (no alembic infrastructure exists in this project).

Create `scripts/migrate_004_category_indicators.py`:
```python
"""
Standalone migration script to add category_indicators column.
Run with: python scripts/migrate_004_category_indicators.py
"""
import sqlite3
import os

def migrate():
    db_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'brasilintel.db')
    if not os.path.exists(db_path):
        print(f"Database not found at {db_path}")
        return

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Check if column already exists
    cursor.execute("PRAGMA table_info(news_items)")
    columns = [col[1] for col in cursor.fetchall()]

    if 'category_indicators' in columns:
        print("category_indicators column already exists")
        conn.close()
        return

    # Add column
    cursor.execute("ALTER TABLE news_items ADD COLUMN category_indicators VARCHAR(500)")
    conn.commit()
    print("Added category_indicators column to news_items table")
    conn.close()

if __name__ == "__main__":
    migrate()
```

Ensure the scripts/ directory exists before creating the file.
  </action>
  <verify>python -c "import ast; ast.parse(open('scripts/migrate_004_category_indicators.py').read()); print('Migration script is valid Python')"</verify>
  <done>Standalone database migration script created for category_indicators column</done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive classification service tests</name>
  <files>tests/test_classifier.py</files>
  <action>
Create comprehensive tests for ClassificationService following existing test patterns:

```python
"""Tests for classification service."""
import pytest
from unittest.mock import Mock, patch, MagicMock
from app.services.classifier import ClassificationService, SYSTEM_PROMPT_SINGLE
from app.schemas.classification import NewsClassification, InsurerClassification


class TestNewsClassificationSchema:
    """Tests for NewsClassification Pydantic model."""

    def test_valid_classification(self):
        """Should accept valid classification data."""
        nc = NewsClassification(
            status="Critical",
            summary_bullets=["Crise financeira detectada"],
            sentiment="negative",
            reasoning="Notícia indica problemas financeiros graves",
            category_indicators=["financial_crisis", "regulatory_action"]
        )
        assert nc.status == "Critical"
        assert nc.sentiment == "negative"
        assert "financial_crisis" in nc.category_indicators

    def test_invalid_status_rejected(self):
        """Should reject invalid status values."""
        with pytest.raises(ValueError):
            NewsClassification(
                status="InvalidStatus",
                summary_bullets=["test"],
                sentiment="neutral",
                reasoning="test"
            )

    def test_invalid_sentiment_rejected(self):
        """Should reject invalid sentiment values."""
        with pytest.raises(ValueError):
            NewsClassification(
                status="Watch",
                summary_bullets=["test"],
                sentiment="invalid_sentiment",
                reasoning="test"
            )

    def test_empty_category_indicators_allowed(self):
        """Should allow empty category_indicators list."""
        nc = NewsClassification(
            status="Stable",
            summary_bullets=["Operações normais"],
            sentiment="neutral",
            reasoning="Sem eventos significativos",
            category_indicators=[]
        )
        assert nc.category_indicators == []

    def test_default_category_indicators(self):
        """Should default to empty list if not provided."""
        nc = NewsClassification(
            status="Stable",
            summary_bullets=["test"],
            sentiment="neutral",
            reasoning="test"
        )
        assert nc.category_indicators == []


class TestClassificationServiceInit:
    """Tests for ClassificationService initialization."""

    def test_init_without_config(self):
        """Should handle missing Azure OpenAI config gracefully."""
        with patch('app.services.classifier.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                is_azure_openai_configured=Mock(return_value=False),
                use_llm_summary=True
            )
            service = ClassificationService()
            assert service.client is None
            assert service.model is None

    def test_init_with_llm_disabled(self):
        """Should respect use_llm_summary=False setting."""
        with patch('app.services.classifier.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                is_azure_openai_configured=Mock(return_value=True),
                use_llm_summary=False,
                azure_openai_endpoint="https://test.openai.azure.com/",
                azure_openai_api_key="test-key",
                azure_openai_api_version="2024-08-01-preview",
                azure_openai_deployment="gpt-4o"
            )
            with patch('app.services.classifier.AzureOpenAI'):
                service = ClassificationService()
                assert service.use_llm is False


class TestClassificationServiceFallback:
    """Tests for fallback classification behavior."""

    def test_fallback_when_client_none(self):
        """Should return fallback when client is None."""
        with patch('app.services.classifier.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                is_azure_openai_configured=Mock(return_value=False),
                use_llm_summary=True
            )
            service = ClassificationService()
            result = service.classify_single_news("Test Insurer", "Test Title")

            assert result is not None
            assert result.status == "Monitor"
            assert result.sentiment == "neutral"
            assert "routine_operations" in result.category_indicators

    def test_fallback_when_llm_disabled(self):
        """Should return fallback when use_llm_summary=False."""
        with patch('app.services.classifier.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                is_azure_openai_configured=Mock(return_value=True),
                use_llm_summary=False,
                azure_openai_endpoint="https://test.openai.azure.com/",
                azure_openai_api_key="test-key",
                azure_openai_api_version="2024-08-01-preview",
                azure_openai_deployment="gpt-4o"
            )
            with patch('app.services.classifier.AzureOpenAI'):
                service = ClassificationService()
                result = service.classify_single_news("Test Insurer", "Test Title")

                assert result is not None
                assert result.status == "Monitor"

    def test_fallback_insurer_classification(self):
        """Should return fallback insurer classification when LLM unavailable."""
        with patch('app.services.classifier.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                is_azure_openai_configured=Mock(return_value=False),
                use_llm_summary=True
            )
            service = ClassificationService()
            result = service.classify_insurer_news("Test", [{"title": "News"}])

            assert result is not None
            assert result.overall_status == "Monitor"
            assert result.sentiment_breakdown == {"positive": 0, "negative": 0, "neutral": 0}


class TestClassificationServiceHealthCheck:
    """Tests for health check functionality."""

    def test_health_check_not_configured(self):
        """Should return error status when not configured."""
        with patch('app.services.classifier.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                is_azure_openai_configured=Mock(return_value=False),
                use_llm_summary=True
            )
            service = ClassificationService()
            health = service.health_check()

            assert health["status"] == "error"
            assert "not configured" in health["message"]

    def test_health_check_disabled(self):
        """Should return disabled status when LLM disabled."""
        with patch('app.services.classifier.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                is_azure_openai_configured=Mock(return_value=True),
                use_llm_summary=False,
                azure_openai_endpoint="https://test.openai.azure.com/",
                azure_openai_api_key="test-key",
                azure_openai_api_version="2024-08-01-preview",
                azure_openai_deployment="gpt-4o"
            )
            with patch('app.services.classifier.AzureOpenAI'):
                service = ClassificationService()
                health = service.health_check()

                assert health["status"] == "disabled"


class TestSystemPrompt:
    """Tests for system prompt content."""

    def test_prompt_includes_classification_criteria(self):
        """System prompt should include all classification criteria."""
        assert "CRITICAL" in SYSTEM_PROMPT_SINGLE
        assert "WATCH" in SYSTEM_PROMPT_SINGLE
        assert "MONITOR" in SYSTEM_PROMPT_SINGLE
        assert "STABLE" in SYSTEM_PROMPT_SINGLE

    def test_prompt_includes_category_indicators(self):
        """System prompt should include category indicator descriptions."""
        prompt_lower = SYSTEM_PROMPT_SINGLE.lower()
        # Check for key category indicators in Portuguese descriptions
        assert "financial_crisis" in prompt_lower or "crise financeira" in prompt_lower
        assert "regulatory_action" in prompt_lower or "ação regulatória" in prompt_lower or "intervenção" in prompt_lower
        assert "m_and_a" in prompt_lower or "fusões" in prompt_lower or "aquisições" in prompt_lower
        assert "leadership_change" in prompt_lower or "liderança" in prompt_lower

    def test_prompt_requests_portuguese_output(self):
        """System prompt should request Portuguese output."""
        assert "português" in SYSTEM_PROMPT_SINGLE.lower()


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```
  </action>
  <verify>cd C:\BrasilIntel && python -m pytest tests/test_classifier.py -v --tb=short</verify>
  <done>Comprehensive tests created covering schema validation, fallback behavior, health check, and prompt content</done>
</task>

<task type="auto">
  <name>Task 3: Run migration and verify database schema</name>
  <files>None (verification task)</files>
  <action>
Run the standalone migration script to add category_indicators column to existing database:

```bash
cd C:\BrasilIntel && python scripts/migrate_004_category_indicators.py
```

Verify column exists:
```python
import sqlite3
conn = sqlite3.connect('data/brasilintel.db')
cursor = conn.cursor()
cursor.execute("PRAGMA table_info(news_items)")
columns = [col[1] for col in cursor.fetchall()]
print("category_indicators" in columns)
conn.close()
```

If the database doesn't exist yet or news_items table doesn't exist, SQLAlchemy will create it with the new column on next startup - no migration needed.
  </action>
  <verify>python -c "import sqlite3; import os; db='C:/BrasilIntel/data/brasilintel.db'; conn=sqlite3.connect(db) if os.path.exists(db) else None; print('DB exists:', os.path.exists(db)); conn and print([c[1] for c in conn.execute('PRAGMA table_info(news_items)').fetchall()])"</verify>
  <done>Database schema updated with category_indicators column (or verified ready for auto-creation)</done>
</task>

</tasks>

<verification>
1. Migration exists: `python -c "import ast; ast.parse(open('scripts/migrate_004_category_indicators.py').read()); print('OK')"`
2. Tests pass: `python -m pytest tests/test_classifier.py -v`
3. Schema validation: All NewsClassification test cases pass
4. Fallback tests: LLM disabled returns Monitor status with routine_operations category
5. Database column: `sqlite3 data/brasilintel.db "PRAGMA table_info(news_items)"` shows category_indicators
</verification>

<success_criteria>
- Standalone migration script exists at scripts/migrate_004_category_indicators.py
- tests/test_classifier.py has 15+ test cases
- All tests pass with `pytest tests/test_classifier.py`
- TestNewsClassificationSchema covers valid/invalid inputs
- TestClassificationServiceFallback verifies USE_LLM_SUMMARY=false behavior
- TestSystemPrompt verifies prompt includes category indicators
- Database has category_indicators column (or will auto-create)
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-classification-pipeline/04-02-SUMMARY.md`
</output>
